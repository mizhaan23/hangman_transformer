{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an encoder only Transformer network for word reconstruciton\n",
    "\n",
    "In this notebook, we will implement the pre-training step where we train our transformer neural network located in the `model` directory to be able to *re-construct* an incomplete word (`_ppl_`) into the complete word (`apple`). This is designed as a superwised learning task where the input is the *masked* word (according to the rules of hangman, i.e. all occurences of a letter would be either masked or shown, exclusively) and the target being the complete word.\n",
    "\n",
    "`MyMasker` is a class under `utils` that implements the desired masking strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V6hm8XxxZFSa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We load the text data, i.e. the list of words and split it randomly for train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Okn0Xe3LCp6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size : 225027, \n",
      "Validation size : 2273\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import MyTokenizer, MyMasker, TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# prepare the dataset\n",
    "dataset = TextDataset('./data/words_250000_train.txt')\n",
    "\n",
    "train_split_percent = 0.99\n",
    "train_size = int(train_split_percent * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "print(f'Training size : {train_size}, \\nValidation size : {test_size}')\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformer network\n",
    "1. We choose a `max_word_length` of $32$, assuming that the length of the longest word is $\\le 32$.\n",
    "2. The `src_vocab` is $26 + 1 + 1 = 28$, i.e. no. of letters of the english alphabet + special token for the blank space `_` + and a padding token `0`, respectively.\n",
    "3. The other hyperparameters were chosen according to convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MtIO-GZjZVxX"
   },
   "outputs": [],
   "source": [
    "from model.Models import Transformer\n",
    "\n",
    "# Loading Tranformer model from scratch\n",
    "max_word_length = 32\n",
    "model = Transformer(src_vocab=28, d_model=128, max_seq_len=max_word_length, N=12, heads=8, dropout=0.1)\n",
    "\n",
    "# Use Xavier initialization\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# We will use Adam optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Before defining the training loop, we showcase the working of the `Masker` and `Tokenizer` utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "example = ['hello', 'world']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h_ll_', 'wo_l_']\n"
     ]
    }
   ],
   "source": [
    "masked_example = masker.mask(example, percentage=.5)  # run this multiple times with different percentage values\n",
    "print(masked_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_ll_\n",
      "[8, 27, 12, 12, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "wo_l_\n",
      "[23, 15, 27, 12, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenized words\n",
    "for a, b in zip(masked_example, tokenizer.encode(masked_example).tolist()):\n",
    "    print(f'{a}\\n{b}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the training loop below ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Rt2HRx-NZMZl"
   },
   "outputs": [],
   "source": [
    "def create_masks(x):\n",
    "    '''This function replaces non pad elements, i.e. letters and blank space with True while pad with False'''\n",
    "    return (x != 0).unsqueeze(-2)\n",
    "\n",
    "def train_model(model, epochs, printevery=1):\n",
    "    \n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        device='cuda'\n",
    "        print('CUDA supported GPU detected! Training now ...')\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print('No CUDA supported GPU detected! Exiting training...')\n",
    "        return\n",
    "    \n",
    "    # Main loop\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for i, trg in enumerate(trainloader):\n",
    "\n",
    "            # src is the masked incomplete word\n",
    "            perc=None\n",
    "            src = masker.mask(trg, perc)  # e.g. ['__llo', 'w_r_d']\n",
    "            src = tokenizer.encode(src)  # e.g. [[ 8, 27, 12, 12, 15,  0, ..., 0], [23, 15, 27, 12,  4,  0,  ..., 0]]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            trg = tokenizer.encode(trg)\n",
    "\n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask = create_masks(src) \n",
    "\n",
    "            # Loading to CUDA GPU\n",
    "            src, mask, trg = src.to(device), mask.to(device), trg.to(device)\n",
    "            \n",
    "            # Predictions are as logits from the model output\n",
    "            preds = model(src)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), trg.contiguous().view(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(trainloader.dataset) * trainloader.batch_size)\n",
    "                avg_loss = total_loss / printevery\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='')\n",
    "                total_loss = 0\n",
    "            \n",
    "            if (i+1) % 10 == 0:  # checkpoint saving\n",
    "                torch.save(model.state_dict(), f'./weights/pretrained_model_weights_epoch{epoch}')\n",
    "                pass\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        torch.save(model.state_dict(), f'./weights/pretrained_model_weights_epoch{epoch}')\n",
    "        \n",
    "        # Run a validation after each epoch\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "        \n",
    "        sims = 0\n",
    "        for i, val in enumerate(valloader):\n",
    "            perc=None\n",
    "            src = masker.mask(val, perc)\n",
    "            src = tokenizer.encode(src)\n",
    "            \n",
    "            val = tokenizer.encode(val)\n",
    "            \n",
    "            mask = create_masks(src)\n",
    "            \n",
    "            # Loading to CUDA GPU\n",
    "            src, mask, val = src.to(device), mask.to(device), val.to(device)\n",
    "            \n",
    "            preds = model(src)\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), val.contiguous().view(-1), ignore_index=0)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            sims += 1\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(valloader.dataset) * valloader.batch_size)\n",
    "                avg_val_loss = total_val_loss / sims\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_val_loss), end='')\n",
    "            \n",
    "        print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, val loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the train function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RZ1vQwRQa5tV",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA supported GPU detected! Training now ...\n",
      "   0m: epoch 1 [                    ]  2%  loss = 1.084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "After successful training of the model, we can use the model predicted probabilites of the missing letters to formulate a winning strategy for the game of Hangman!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "trexquant_challenge.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
