{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7043409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from model.Models import Transformer\n",
    "from model.Optim import CosineWithRestarts\n",
    "from model.Batch import create_masks\n",
    "from utils.utils import MyTokenizer, MyMasker\n",
    "from utils.data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c36703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225027 2273\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "bs=128\n",
    "dataset = TextDataset()\n",
    "train_size = int(0.99*len(dataset))\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9badf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(32)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a59eb",
   "metadata": {},
   "source": [
    "### Implementing custom Hangman gym-based env\n",
    "* Follows the gym protocol.\n",
    "* Is vectorized and can support multithreading for parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca20ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dataloader, max_seq_len=32, init_counter=0):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        self.dataset = shuffle(dataloader.dataset)\n",
    "        self.counter = init_counter\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.action_space = spaces.Discrete(28)  # 26 possible actions (a-z) + '' + '_'\n",
    "        self.observation_space = spaces.Box(low=0, high=27, shape=(self.max_seq_len,), dtype=int)\n",
    "\n",
    "        self.hidden_word = None\n",
    "        self.word_length = None\n",
    "        self._reset_attributes()\n",
    "    \n",
    "    def _reset_attributes(self):\n",
    "        self.guessed_letters = set()\n",
    "        self.remaining_attempts = 6  # Maximum attempts\n",
    "        self.current_state = np.zeros(self.max_seq_len, dtype=int)  # Initial state\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self, *, seed=0, options=None):\n",
    "        self.hidden_word = self.dataset[self.counter % len(self.dataset)]\n",
    "        self.word_length = len(self.hidden_word)\n",
    "        self._reset_attributes()\n",
    "        \n",
    "        # Increment reset counter\n",
    "        self.counter += 1\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, \\\n",
    "            {\n",
    "            'word': current_word, \n",
    "            'hidden_word': self.hidden_word, \n",
    "            'guessed_letters': self.guessed_letters,\n",
    "            'remaining_attempts': self.remaining_attempts,\n",
    "        }\n",
    "\n",
    "    def generate_random_word(self):\n",
    "        # Replace this with your logic for generating random words\n",
    "        word_list = self.dataset\n",
    "        idx = self.counter % len(word_list)\n",
    "        self.counter += 1\n",
    "        return word_list[idx]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in self.guessed_letters:\n",
    "            print(\"You have already guessed that letter.\")\n",
    "        else:\n",
    "            self.guessed_letters.add(action)\n",
    "            if action in self.hidden_word:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 0\n",
    "                self.remaining_attempts -= 1\n",
    "\n",
    "        if set(self.hidden_word) <= self.guessed_letters or self.remaining_attempts == 0:\n",
    "            reward = 1 if set(self.hidden_word) <= self.guessed_letters else 0\n",
    "            self.game_over = True\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, reward, self.game_over, self.game_over, \\\n",
    "            {\n",
    "            'word': current_word, \n",
    "            'hidden_word': self.hidden_word, \n",
    "            'guessed_letters': self.guessed_letters,\n",
    "            'remaining_attempts': self.remaining_attempts,\n",
    "        }\n",
    "\n",
    "    def word2state(self, word):\n",
    "        state = [27 if char == '_' else ord(char) - ord('a') + 1 for char in word]\n",
    "        while len(state) < self.max_seq_len:\n",
    "            state.append(0)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d9444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(guessed_letters):\n",
    "    \n",
    "    valid_actions = torch.ones((len(guessed_letters), 28)).to('cuda')\n",
    "    valid_actions[:,  0] = 0.\n",
    "    valid_actions[:, -1] = 0.\n",
    "    \n",
    "    for i, s in enumerate(guessed_letters):\n",
    "        for char in s:\n",
    "            idx = ord(char) - ord('a') + 1\n",
    "            valid_actions[i, idx] = 0.\n",
    "    \n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551ad7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, envs, buffer_size):\n",
    "        \n",
    "        self.envs = envs\n",
    "#         self.memory = deque(maxlen=buffer_size)\n",
    "#         self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "#         self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "#         self.seed = random.seed(seed)\n",
    "        \n",
    "        # pointer\n",
    "        self.t_ = 0\n",
    "        \n",
    "        # Initializing simulation matrices for the given batched episode\n",
    "        n = envs.num_envs\n",
    "        self.states = torch.zeros((buffer_size, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "        self.numlives = torch.zeros((buffer_size, n), dtype=int).to(device)\n",
    "        self.actions = torch.zeros((buffer_size, n), dtype=int).to(device)\n",
    "        self.rewards = torch.zeros((buffer_size, n), dtype=torch.float32).to(device)\n",
    "        self.next_states = torch.zeros((buffer_size, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "        self.next_valid_actions = torch.zeros((buffer_size, n, envs.single_action_space.n), dtype=int).to(device)\n",
    "        self.next_numlives = torch.zeros((buffer_size, n), dtype=int).to(device)\n",
    "        self.dones = torch.ones((buffer_size, n), dtype=bool).to(device)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, next_valid_action, numlives, next_numlives, done):\n",
    "        idx = self.t_ % self.buffer_size\n",
    "        \n",
    "        self.states[idx] = state\n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = torch.tensor(reward).to(device)\n",
    "        self.next_states[idx] = next_state\n",
    "        self.next_valid_actions[idx] = next_valid_action\n",
    "        self.numlives[idx] = torch.tensor(numlives).to(device)\n",
    "        self.next_numlives[idx] = torch.tensor(next_numlives).to(device)\n",
    "        self.dones[idx] = torch.tensor(done).to(device)\n",
    "        self.t_ += 1\n",
    "    \n",
    "    def sample(self):\n",
    "        \n",
    "        # Flatten the simulation matrices\n",
    "        b_states = self.states.reshape((-1,) + self.envs.single_observation_space.shape)\n",
    "        b_actions = self.actions.reshape(-1)\n",
    "        b_rewards = self.rewards.reshape(-1)\n",
    "        b_next_states = self.next_states.reshape((-1,) + self.envs.single_observation_space.shape)\n",
    "        b_next_valid_actions = self.next_valid_actions.reshape((-1, self.envs.single_action_space.n))\n",
    "        b_numlives = self.numlives.reshape(-1)\n",
    "        b_next_numlives = self.next_numlives.reshape(-1)\n",
    "        b_dones = self.dones.reshape(-1)\n",
    "        \n",
    "        return (b_states, b_actions, b_rewards, b_next_states, b_next_valid_actions, b_numlives, b_next_numlives, b_dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99903d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BehaviourPolicy(nn.Module):\n",
    "    def __init__(self, envs, temperature=1.):\n",
    "        super(BehaviourPolicy, self).__init__()\n",
    "        \n",
    "        self.memory = ReplayBuffer(envs, buffer_size=32)\n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.pretrainedLLM = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrainedLLM.load_state_dict(torch.load('./weights/model_weights_epoch750_04042024'))\n",
    "        \n",
    "        # helper function\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # temperature\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.update_every = 1\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, next_valid_action, numlives, next_numlives, done):    \n",
    "        self.memory.add(state, action, reward, next_state, next_valid_action, numlives, next_numlives, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            pass\n",
    "\n",
    "    def act(self, x, valid_actions):\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x) / self.temperature\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        probs = torch.matmul(1.*mask, probs)  # effectively adds the probs row-wise for each action / character\n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs / torch.sum(probs)\n",
    "\n",
    "        fprobs = torch.mul(probs, valid_actions)\n",
    "        fprobs = fprobs / torch.sum(fprobs)        \n",
    "\n",
    "#         dist = Categorical(probs=fprobs)\n",
    "#         action = dist.sample()\n",
    "#         logprob = dist.log_prob(action)\n",
    "        \n",
    "        action = torch.argmax(fprobs, dim=-1)\n",
    "        \n",
    "        return action, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6e01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.uniform_(layer.weight)\n",
    "#     torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, probs=None, logits=None, validate_args=None, masks=[]):\n",
    "        \n",
    "        self.masks = masks\n",
    "        if len(self.masks) == 0:\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "        else:\n",
    "            self.masks = masks.type(torch.BoolTensor).to(device)\n",
    "            logits = torch.where(self.masks, logits, torch.tensor(-1e8).to(device))\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "\n",
    "    def entropy(self):\n",
    "        if len(self.masks) == 0:\n",
    "            return super(CategoricalMasked, self).entropy()\n",
    "        p_log_p = self.logits * self.probs\n",
    "        p_log_p = torch.where(self.masks, p_log_p, torch.tensor(0.0).to(device))\n",
    "        return -p_log_p.sum(-1)\n",
    "    \n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.transformer = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        \n",
    "        self.pretrained = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrained.load_state_dict(torch.load('./weights/model_weights_epoch750_04042024'))\n",
    "        \n",
    "        self.pretrained.eval()\n",
    "        \n",
    "        # Flatten output logits from transformer and feed into a LINEAR LAYER\n",
    "        self.output = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 28, bias=False)),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.scalar = nn.Parameter(torch.tensor(0.))\n",
    "        self.matrix = nn.Parameter(torch.ones((6, 32, 28)))\n",
    "        \n",
    "    def forward(self, x, stages):\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        with torch.no_grad():\n",
    "            h = self.pretrained(x)\n",
    "            h = nn.functional.softmax(h, dim=-1)\n",
    "#         x = nn.functional.softmax(self.transformer(x), dim=-1)    \n",
    "        x = h\n",
    "        x_masked = x * torch.stack((mask.squeeze(-2),) * x.shape[-1], dim=-1)\n",
    "        \n",
    "#         mean = x_masked.mean(dim=-2)\n",
    "        \n",
    "#         x_masked_flatten = x_masked.view((x_masked.shape[0], -1))\n",
    "        \n",
    "#         mu = nn.functional.sigmoid(self.scalar)\n",
    "        \n",
    "#         print(self.output(x_masked_flatten).shape)\n",
    "#         print(mean.shape)\n",
    "\n",
    "        return (x_masked * self.matrix[stages-1, ...]).sum(dim=-2)\n",
    "    \n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        self.gamma = 1.00\n",
    "        self.tau = 1e-1\n",
    "        self.lr = 1e-2\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(envs).to(device)\n",
    "        self.qnetwork_target = QNetwork(envs).to(device)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.qnetwork_local.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.memory = ReplayBuffer(envs, buffer_size=32)\n",
    "        \n",
    "        self.buffer_size = int(1e5)\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.update_every = 1\n",
    "        self.t_step = 0\n",
    "        \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, next_valid_action, numlives, next_numlives, done):    \n",
    "        self.memory.add(state, action, reward, next_state, next_valid_action, numlives, next_numlives, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            pass\n",
    "#             if len(self.memory) > self.batch_size:\n",
    "#             experiences = self.memory.sample()\n",
    "#             self.learn(experiences)\n",
    "                \n",
    "    \n",
    "    def act(self, state, valid_actions, numlives, eps=0.):\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state, numlives)\n",
    "        self.qnetwork_local.train()\n",
    "        actions = torch.argmax(action_values * valid_actions, dim=-1)\n",
    "        return actions, None\n",
    "        \n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, next_valid_actions, numlives, next_numlives, dones = experiences\n",
    "        \n",
    "        Q_targets_next = (self.qnetwork_target(next_states, next_numlives).detach() * next_valid_actions).max(1)[0]\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - 1.*dones))\n",
    "        Q_targets = Q_targets.unsqueeze(-1)\n",
    "        \n",
    "        Q_expected = self.qnetwork_local(states, numlives).gather(1, actions.unsqueeze(-1))\n",
    "        \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfb65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dad274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the environment\n",
    "dataloader = valloader\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: HangmanEnv(dataloader) for i in range(dataloader.batch_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10055471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviourPolicy(\n",
       "  (pretrainedLLM): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (embed): Embedder(\n",
       "        (embed): Embedding(28, 128, padding_idx=0)\n",
       "      )\n",
       "      (pe): PositionalEncoder(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x EncoderLayer(\n",
       "          (norm_1): Norm()\n",
       "          (norm_2): Norm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (linear_1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Norm()\n",
       "    )\n",
       "    (out): Linear(in_features=128, out_features=28, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "# agent = DQNAgent(envs).to(device)\n",
    "agent = BehaviourPolicy(envs, temperature=1.).to(device)\n",
    "\n",
    "# agent.qnetwork_local.load_state_dict(torch.load('online-linear-DQN-checkpoint.pth'))\n",
    "\n",
    "agent.eval()\n",
    "# behaviourPolicy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b575bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Create a deque with a maximum length of 20\n",
    "max_size = 10\n",
    "losses = deque(maxlen=max_size)\n",
    "\n",
    "def dqn(n_episodes=1000):\n",
    "#     scores = []\n",
    "#     scores_window = deque(maxlen=100)\n",
    "    \n",
    "    wins = total_games = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    i_episode = 0\n",
    "    while True:\n",
    "        \n",
    "        state, info = envs.reset()\n",
    "        state = torch.tensor(state).to(device)\n",
    "        \n",
    "        for t in range(agent.memory.buffer_size):\n",
    "            valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "            numlives = info['remaining_attempts']\n",
    "            \n",
    "            if np.random.rand() < 1.:\n",
    "                action_ints, _ = agent.act(state, valid_actions)\n",
    "                \n",
    "            else:  # follow random policy for exploration\n",
    "                action_ints = []\n",
    "                for v in valid_actions:\n",
    "                    v_ = v.nonzero().flatten()\n",
    "                    idx = torch.randint(0, v_.numel(), (1,)).item()\n",
    "                    action_ints.append(v_[idx])\n",
    "                action_ints = torch.tensor(action_ints).to(device)\n",
    "            \n",
    "            # Convert to action_int to action_str guesses\n",
    "            action_strs =  [chr(idx-1 + ord('a')) for idx in action_ints]\n",
    "            \n",
    "            # Take step in the envs\n",
    "            next_state, reward, terminated, truncated, info = envs.step(action_strs)\n",
    "            next_state = torch.tensor(next_state).to(device)\n",
    "            done = (terminated | truncated)\n",
    "            \n",
    "            agent.step(state, action_ints, reward, next_state, \\\n",
    "                       get_valid_actions(info['guessed_letters']), numlives, info['remaining_attempts'], done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        experiences = agent.memory.sample()\n",
    "        loss = torch.tensor(0)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        wins += agent.memory.rewards.sum()\n",
    "        total_games += agent.memory.dones.sum()\n",
    "        \n",
    "        win_rate = wins / total_games\n",
    "        curr_win_rate = agent.memory.rewards.sum() / agent.memory.dones.sum()\n",
    "        \n",
    "        mean_time_per_game = (time.time() - start_time) / total_games\n",
    "        \n",
    "        print('''\\r  loss = %.03f \\t wins : %d \\t total games : %d \\t win rate : %.03f%% \\t curr win rate : %.03f%% \\t time_per_game : %.03f ms''' \\\n",
    "              %(sum(losses) / len(losses), wins, total_games, 100*win_rate, 100*curr_win_rate, 1000*mean_time_per_game), end='', flush=True)\n",
    "        \n",
    "        i_episode += 1\n",
    "        \n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c4da33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loss = 0.000 \t wins : 482152 \t total games : 730610 \t win rate : 65.993% \t curr win rate : 66.964% \t time_per_game : 4.515 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "scores = dqn()\n",
    "\n",
    "# Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = saved_agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
