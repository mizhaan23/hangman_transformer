{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V6hm8XxxZFSa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.Models import Transformer, Transformer2\n",
    "from model.Optim import CosineWithRestarts\n",
    "from model.Batch import create_masks\n",
    "from utils.utils import MyTokenizer, MyMasker\n",
    "from utils.data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Okn0Xe3LCp6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225027 2273\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "bs=128\n",
    "dataset = TextDataset()\n",
    "train_size = int(0.99*len(dataset))\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ed2KNafObWfJ"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(32)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MtIO-GZjZVxX"
   },
   "outputs": [],
   "source": [
    "# Loading Tranformer model from scratch\n",
    "max_len = 32\n",
    "model = Transformer(src_vocab=28, d_model=128, max_seq_len=max_len, N=12, heads=8, dropout=0.1)\n",
    "model.to('cuda')\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Lut9a2Jyk2Q"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(max_len)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rt2HRx-NZMZl"
   },
   "outputs": [],
   "source": [
    "def train_model(model, bs, epochs, printevery):\n",
    "\n",
    "    print(\"training model...\")\n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        print('gpu detected!')\n",
    "    else:\n",
    "        print('no gpu detected')\n",
    "        return 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, trg in enumerate(trainloader):\n",
    "\n",
    "            # src = batch.src.transpose(0,1)\n",
    "            # trg = batch.trg.transpose(0,1)\n",
    "            # trg_input = trg[:, :-1]\n",
    "            # src_mask, _ = create_masks(src, trg_input) # need to edit\n",
    "\n",
    "            # test to check if overfit\n",
    "\n",
    "            # src is the incomplete word\n",
    "            perc=None\n",
    "            src = masker.mask(trg, perc)  # e.g. [m_zh__n, _s, _w_so_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            trg = tokenizer.encode(trg)\n",
    "\n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "\n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                trg = trg.to('cuda')\n",
    "            \n",
    "            model.train()\n",
    "            # preds = model(src, mask)\n",
    "            preds = model(src)\n",
    "            # ys = trg[:, 1:].contiguous().view(-1)\n",
    "            # y = mask.squeeze(1)\n",
    "            \n",
    "            # \n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), trg.contiguous().view(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print(i+1)\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(trainloader.dataset) * bs)\n",
    "                avg_loss = total_loss / printevery\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='')\n",
    "                total_loss = 0\n",
    "\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), f'./weights/model_automask_weights_{datetime.today().strftime(\"%m%d%Y\")}')\n",
    "                pass\n",
    "                \n",
    "        total_val_loss = 0\n",
    "        sims = 0\n",
    "        for i, val in enumerate(valloader):\n",
    "            perc=None\n",
    "            src = masker.mask(val, perc)  # e.g. [m_zh__n, _s, _w_eso_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            val = tokenizer.encode(val)\n",
    "            \n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "            \n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                val = val.to('cuda')\n",
    "            \n",
    "            model.eval()\n",
    "            preds = model(src)\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), val.contiguous().view(-1), ignore_index=0)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            sims += 1\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(valloader.dataset) * bs)\n",
    "                avg_val_loss = total_val_loss / sims\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_val_loss), end='')\n",
    "            \n",
    "        print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, val loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RZ1vQwRQa5tV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_model(model, bs=bs, epochs=25, printevery=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trajectories(envs, policy, horizon, device):\n",
    "    n = envs.num_envs\n",
    "\n",
    "    # Initializing simulation matrices for the given batched episode\n",
    "    log_probs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "\n",
    "    obs, _ = envs.reset()\n",
    "    done = np.zeros((n,), dtype=bool)  # e.g. [False, False, False]\n",
    "    T = None\n",
    "\n",
    "    for t in range(horizon):\n",
    "        obs = torch.tensor(np.float32(obs)).to(device)\n",
    "\n",
    "        action, log_prob = policy.get_action(obs)\n",
    "\n",
    "        log_probs[t] = log_prob\n",
    "        dones[t] = torch.tensor(done).to(device)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = envs.step(action.cpu().detach().numpy())\n",
    "        done = done | (np.array(terminated) | np.array(truncated))\n",
    "\n",
    "        # Modify rewards to NOT consider data points after `done`\n",
    "        reward = reward * ~done\n",
    "        rewards[t] = torch.tensor(reward).to(device)\n",
    "\n",
    "        if done.all():\n",
    "            T = t\n",
    "            break\n",
    "\n",
    "    cum_discounted_rewards = discount_cumsum(rewards, dones, gamma=0.99, normalize=False, device=device)\n",
    "    mean_episode_return = torch.sum(cum_discounted_rewards, axis=0) / torch.sum(~dones, axis=0)\n",
    "\n",
    "    traj_info = {\n",
    "        'log_probs': log_probs[:T],\n",
    "        'rewards': rewards[:T],\n",
    "        'dones': dones[:T],\n",
    "    }\n",
    "\n",
    "    return traj_info, torch.sum(rewards, axis=0), mean_episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand((3, 28, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dataloader, max_seq_len=32, init_counter=0):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        self.dataset = shuffle(dataloader.dataset)\n",
    "        self.counter = init_counter\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.action_space = spaces.Discrete(28)  # 26 possible actions (a-z) + '' + '_'\n",
    "        self.observation_space = spaces.Box(low=0, high=27, shape=(self.max_seq_len,), dtype=int)\n",
    "\n",
    "        self.hidden_word = None\n",
    "        self.word_length = None\n",
    "        self._reset_attributes()\n",
    "    \n",
    "    def _reset_attributes(self):\n",
    "        self.guessed_letters = set()\n",
    "        self.remaining_attempts = 6  # Maximum attempts\n",
    "        self.current_state = np.zeros(self.max_seq_len, dtype=int)  # Initial state\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self, *, seed=0, options=None):\n",
    "        self.hidden_word = self.dataset[self.counter % len(self.dataset)]\n",
    "        self.word_length = len(self.hidden_word)\n",
    "        self._reset_attributes()\n",
    "        \n",
    "        # Increment reset counter\n",
    "        self.counter += 1\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def generate_random_word(self):\n",
    "        # Replace this with your logic for generating random words\n",
    "        word_list = self.dataset\n",
    "        idx = self.counter % len(word_list)\n",
    "        self.counter += 1\n",
    "        return word_list[idx]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in self.guessed_letters:\n",
    "            print(\"You have already guessed that letter.\")\n",
    "        else:\n",
    "            self.guessed_letters.add(action)\n",
    "            if action in self.hidden_word:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 0\n",
    "                self.remaining_attempts -= 1\n",
    "\n",
    "        if set(self.hidden_word) <= self.guessed_letters or self.remaining_attempts == 0:\n",
    "            reward = 1 if set(self.hidden_word) <= self.guessed_letters else 0\n",
    "            self.game_over = True\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, reward, self.game_over, self.game_over, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def word2state(self, word):\n",
    "        state = [27 if char == '_' else ord(char) - ord('a') + 1 for char in word]\n",
    "        while len(state) < self.max_seq_len:\n",
    "            state.append(0)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(guessed_letters):\n",
    "    \n",
    "    valid_actions = torch.ones((len(guessed_letters), 28)).to('cuda')\n",
    "    valid_actions[:,  0] = 0.\n",
    "    valid_actions[:, -1] = 0.\n",
    "    \n",
    "    for i, s in enumerate(guessed_letters):\n",
    "        for char in s:\n",
    "            idx = ord(char) - ord('a') + 1\n",
    "            valid_actions[i, idx] = 0.\n",
    "    \n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, probs=None, logits=None, validate_args=None, masks=[]):\n",
    "        \n",
    "        self.masks = masks\n",
    "        if len(self.masks) == 0:\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "        else:\n",
    "            self.masks = masks.type(torch.BoolTensor).to(device)\n",
    "            logits = torch.where(self.masks, logits, torch.tensor(-1e8).to(device))\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "\n",
    "    def entropy(self):\n",
    "        if len(self.masks) == 0:\n",
    "            return super(CategoricalMasked, self).entropy()\n",
    "        p_log_p = self.logits * self.probs\n",
    "        p_log_p = torch.where(self.masks, p_log_p, torch.tensor(0.0).to(device))\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, temperature=1.):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.pretrainedLLM = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrainedLLM.load_state_dict(torch.load('./weights/model_weights_epoch750_04042024'))\n",
    "        \n",
    "        # helper function\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Flatten output logits from transformer and feed into feed-forward NN\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 28), std=0.01),\n",
    "        )\n",
    "        \n",
    "        # temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def get_value(self, x):\n",
    "#         with torch.no_grad():\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "        probs_masked_flatten = probs_masked.view((probs_masked.shape[0], -1))\n",
    "            \n",
    "        return self.critic(probs_masked_flatten.detach())\n",
    "\n",
    "    def get_action_and_value(self, x, valid_actions, action=None):\n",
    "#         with torch.no_grad():\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x) / self.temperature\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "        probs_masked_flatten = probs_masked.view((probs_masked.shape[0], -1))\n",
    "\n",
    "#             print(logits)\n",
    "#             print(probs)\n",
    "\n",
    "#             q = (1-probs+1e-9).log()\n",
    "#             q = torch.matmul(1.*mask, q)\n",
    "#             q = q.squeeze(1)\n",
    "# #             print('before', torch.isnan(q).any().item())\n",
    "# #             print(q)\n",
    "#             q = torch.mul(q, valid_actions)\n",
    "# #             q = q / torch.sum(q)\n",
    "# #             print(valid_actions*1)\n",
    "# #             print('after', torch.isnan(q).any().item())\n",
    "# #             print(q)\n",
    "# #             print('----------------------')\n",
    "#             action_new = torch.argmin(q, dim=-1)\n",
    "\n",
    "#             print(probs)\n",
    "#             print(torch.isnan((1-probs).log()).any().item())\n",
    "#             print(torch.min(q, dim=-1))\n",
    "\n",
    "        probs = torch.matmul(1.*mask, probs)  # effectively adds the probs row-wise for each action / character\n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs / torch.sum(probs)\n",
    "\n",
    "#         x = self.pretrainedLLM(x)\n",
    "#         temp = nn.functional.softmax(x, dim=-1)\n",
    "#         print(temp)\n",
    "#         x = x.view((x.shape[0], -1))\n",
    "        \n",
    "#         logits = self.actor(x)\n",
    "#         probs = CategoricalMasked(logits=logits, masks=valid_actions)\n",
    "#         if action is None:\n",
    "#             action = probs.sample()\n",
    "#         logprob = probs.log_prob(action)\n",
    "#         entropy = probs.entropy()\n",
    "\n",
    "        fprobs = torch.mul(probs, valid_actions)\n",
    "        fprobs = fprobs / torch.sum(fprobs)\n",
    "\n",
    "        # Choose max probable actions as int form\n",
    "        action = torch.argmax(fprobs, dim=-1)\n",
    "#         dist = Categorical(probs=fprobs)\n",
    "#         action = dist.sample()\n",
    "        logprob = torch.max(fprobs, dim=-1)[0].log()\n",
    "#         entropy = -torch.where(valid_actions.type(torch.BoolTensor).to(device), fprobs*fprobs.log(), torch.tensor(0.0).to(device))\n",
    "#         entropy = entropy.sum(-1)\n",
    "#         logprob = dist.log_prob(action)\n",
    "#         entropy = dist.entropy()\n",
    "\n",
    "#         print(action.shape, logprob.shape, entropy.shape, self.critic(x).shape)\n",
    "#         print(action.shape, logprob.shape, entropy.shape, self.critic(logits.view((logits.shape[0], -1))).shape)\n",
    "#         print(action)\n",
    "        \n",
    "        return action, logprob, fprobs * 0., self.critic(probs_masked_flatten.detach()) * 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim(agent, envs, optimizer, train=False):\n",
    "    \n",
    "    n = envs.num_envs\n",
    "    horizon = 32\n",
    "    batch_size = horizon*n\n",
    "    minibatch_size = batch_size // 4\n",
    "    num_updates = 10000000 // batch_size\n",
    "    \n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Initializing agent\n",
    "#     agent = Agent(envs).to(device)\n",
    "#     optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
    "    \n",
    "    # Initializing simulation matrices for the given batched episode\n",
    "    observations = torch.zeros((horizon, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "    actions = torch.zeros((horizon, n)).to(device)\n",
    "#     action_masks = torch.zeros((horizon, n) + (envs.single_action_space.n,)).to(device)\n",
    "#     values = torch.ones((horizon, n), dtype=torch.float).to(device)\n",
    "    logprobs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "    \n",
    "    \n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_t = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, info = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros((n,)).to(device)\n",
    "    \n",
    "    valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "    \n",
    "    done = False\n",
    "    cr = 0.\n",
    "    \n",
    "    wins = 0\n",
    "    total_games = 0\n",
    "#     for update in range(1, num_updates + 1):\n",
    "    while True:\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            global_t += 1 * n\n",
    "            observations[t] = next_obs\n",
    "            dones[t] = next_done\n",
    "#             action_masks[t] = valid_actions\n",
    "            \n",
    "#             # Get action probs\n",
    "#             probs = pgn(next_obs)\n",
    "            \n",
    "#             # Zero out invalid actions\n",
    "#             b_probs = torch.mul(probs, valid_actions)\n",
    "#             b_probs = b_probs / torch.sum(b_probs)\n",
    "        \n",
    "#             # Choose max probable actions as int form\n",
    "#             action_ints = torch.argmax(b_probs, dim=-1)\n",
    "            \n",
    "             # ALGO LOGIC: action logic\n",
    "#             with torch.no_grad():\n",
    "            action_ints, logprob, _, _ = agent.get_action_and_value(next_obs, valid_actions)\n",
    "#             values[t] = value.flatten()\n",
    "            actions[t] = action_ints\n",
    "            logprobs[t] = logprob\n",
    "        \n",
    "            # Convert to action_int to action_str guesses\n",
    "            action_strs =  [chr(idx-1 + ord('a')) for idx in action_ints]\n",
    "            \n",
    "            \n",
    "            # Take step in the envs\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action_strs)\n",
    "            done = (terminated | truncated)\n",
    "            \n",
    "            # log data\n",
    "            rewards[t] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).to(device)\n",
    "            next_done = torch.tensor(done).to(device)\n",
    "            \n",
    "            valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "        \n",
    "        wins += rewards.sum()\n",
    "        total_games += dones.sum()\n",
    "        \n",
    "        win_rate = wins / total_games\n",
    "        curr_win_rate = rewards.sum() / dones.sum()\n",
    "        \n",
    "        mean_time_per_game = (time.time() - start_time) / total_games \n",
    "        \n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t curr win rate : %.03f%% \\t time_per_game : %.03f ms' \\\n",
    "              %(wins, total_games, 100*win_rate, 100*curr_win_rate, 1000*mean_time_per_game), end='')\n",
    "        \n",
    "#         print(rewards)\n",
    "#         print()\n",
    "#         print(dones)\n",
    "        \n",
    "        if train:\n",
    "            gamma = 1.\n",
    "            gae_lambda = 0.95\n",
    "            clip_coef = 0.1\n",
    "            ent_coef = 0.01\n",
    "            vf_coef = 0.5\n",
    "            max_grad_norm = 0.5\n",
    "\n",
    "            # bootstrap value if not done\n",
    "            with torch.no_grad():\n",
    "                next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                if False: # if args.gae:\n",
    "                    advantages = torch.zeros_like(rewards).to(device)\n",
    "                    lastgaelam = 0\n",
    "                    for t in reversed(range(horizon)):\n",
    "                        if t == horizon - 1:\n",
    "                            nextnonterminal = 1.0 - 1.*next_done\n",
    "                            nextvalues = next_value\n",
    "                        else:\n",
    "                            nextnonterminal = 1.0 - 1.*dones[t + 1]\n",
    "                            nextvalues = values[t + 1]\n",
    "                        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                    returns = advantages + values\n",
    "                else:\n",
    "                    returns = torch.zeros_like(rewards).to(device)\n",
    "                    for t in reversed(range(horizon)):\n",
    "                        if t == horizon - 1:\n",
    "                            nextnonterminal = 1.0 - 1.*next_done\n",
    "                            next_return = 0. # next_value\n",
    "                        else:\n",
    "                            nextnonterminal = 1.0 - 1.*dones[t + 1]\n",
    "                            next_return = returns[t + 1]\n",
    "                        returns[t] = rewards[t] + gamma * nextnonterminal * next_return\n",
    "                    advantages = returns #- values\n",
    "                    \n",
    "            # flatten the batch\n",
    "#             b_obs = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
    "#             b_logprobs = logprobs.reshape(-1)\n",
    "#             b_actions = actions.reshape(-1)\n",
    "#             b_advantages = advantages.reshape(-1)\n",
    "#             b_returns = returns.reshape(-1)\n",
    "#             b_values = values.reshape(-1)\n",
    "#             b_action_masks = action_masks.reshape((-1, action_masks.shape[-1]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = (logprobs * returns).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Initializing simulation matrices for the given batched episode\n",
    "        observations = torch.zeros((horizon, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "        actions = torch.zeros((horizon, n)).to(device)\n",
    "#         action_masks = torch.zeros((horizon, n) + (envs.single_action_space.n,)).to(device)\n",
    "#         values = torch.ones((horizon, n), dtype=torch.float).to(device)\n",
    "        logprobs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "        rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "        dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "            \n",
    "            \n",
    "#             # Optimizing the policy and value network\n",
    "#             b_inds = np.arange(batch_size)\n",
    "#             clipfracs = []\n",
    "#             for epoch in range(10):\n",
    "#                 np.random.shuffle(b_inds)\n",
    "#                 for start in range(0, batch_size, minibatch_size):\n",
    "#                     end = start + minibatch_size\n",
    "#                     mb_inds = b_inds[start:end]\n",
    "\n",
    "#                     _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
    "#                         b_obs[mb_inds],\n",
    "#                         b_action_masks[mb_inds],\n",
    "#                         b_actions.long()[mb_inds].T,\n",
    "#                     )\n",
    "#                     logratio = newlogprob #- b_logprobs[mb_inds]\n",
    "#                     ratio = logratio #.exp()\n",
    "\n",
    "#                     with torch.no_grad():\n",
    "#                         # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "#                         old_approx_kl = (-logratio).mean()\n",
    "#                         approx_kl = ((ratio - 1) - logratio).mean()\n",
    "#                         clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "#                     mb_advantages = b_advantages[mb_inds]\n",
    "#                     if False: # if args.norm_adv:\n",
    "#                         mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "#                     # Policy loss\n",
    "#                     pg_loss1 = -mb_advantages * ratio\n",
    "#                     pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "#                     pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "#                     # Value loss\n",
    "#                     newvalue = newvalue.view(-1)\n",
    "#                     if True:  # if clip_vloss:\n",
    "#                         v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "#                         v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "#                             newvalue - b_values[mb_inds],\n",
    "#                             -clip_coef,\n",
    "#                             clip_coef,\n",
    "#                         )\n",
    "#                         v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "#                         v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "#                         v_loss = 0.5 * v_loss_max.mean()\n",
    "#                     else:\n",
    "#                         v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "#                     entropy_loss = entropy.mean()\n",
    "#                     loss = pg_loss1.mean() # vf_coef * v_loss #- ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "#                     optimizer.zero_grad()\n",
    "#                     loss.backward()\n",
    "#                     nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "#                     optimizer.step()\n",
    "\n",
    "#                 if False: # if args.target_kl is not None:\n",
    "#                     if approx_kl > target_kl:\n",
    "#                         break\n",
    "\n",
    "#             y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "#             var_y = np.var(y_true)\n",
    "#             explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "            \n",
    "\n",
    "        \n",
    "    envs.close()\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#         [lambda: HangmanEnv(trainloader) for i in range(trainloader.batch_size)]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wins : 128 \t total games : 182 \t win rate : 70.330% \t curr win rate : 69.072% \t time_per_game : 80.324 ms"
     ]
    }
   ],
   "source": [
    "# from env.hangman import HangmanEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "# agent = Agent(envs).to(device)\n",
    "# optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-5, eps=1e-5)\n",
    "\n",
    "# def test_pgn(dataloader):\n",
    "dataloader = trainloader\n",
    "    \n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: HangmanEnv(dataloader) for i in range(dataloader.batch_size)]\n",
    ")\n",
    "\n",
    "agent = Agent(envs, temperature=1.).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "cr = mini_sim(agent, envs, optimizer, train=True)\n",
    "\n",
    "#         print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_ = time.time()\n",
    "test_pgn(trainloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(idx):\n",
    "    def thunk():\n",
    "        env = HangmanEnv(dataloader=valloader, init_counter=0)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda: HangmanEnv(valloader) for i in range(3)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "obs, info = envs.reset()\n",
    "print(torch.tensor(obs))\n",
    "print(get_valid_actions(info['guessed_letters']))\n",
    "done = False\n",
    "while not done:\n",
    "#     action = ['a', 'b', 'b']\n",
    "    action = input(\"Enter a letter to guess: \")\n",
    "    obs, reward, terminated, truncated, info = envs.step(action)\n",
    "    print(info['guessed_letters'])\n",
    "    print(get_valid_actions(info['guessed_letters']))\n",
    "    print(info['guessed_letters'].shape)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim2(sample):\n",
    "    env = HangmanEnv(sample[0])\n",
    "    n = len(sample[0])\n",
    "    state = masker.mask(sample, 1)\n",
    "    sample_mask, _ = create_masks(tokenizer.encode(sample))\n",
    "    mask = sample_mask.to('cuda')\n",
    "    y = sample_mask.squeeze(1).to('cuda')\n",
    "    y_float = torch.where(y, 1., 0.)\n",
    "    \n",
    "    left = torch.ones((1, 28)).to('cuda')\n",
    "    left[0,  0] = 0.\n",
    "    left[0, -1] = 0.\n",
    "    \n",
    "    P = nn.Softmax(dim=-1)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    cr = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # print(state)\n",
    "        \n",
    "        state = tokenizer.encode(state)\n",
    "        state = state.to('cuda')\n",
    "        \n",
    "        # q_probs = score / torch.sum(score)\n",
    "        \n",
    "        probs = pgn(state, mask)\n",
    "        \n",
    "        b_probs = torch.mul(probs, left)\n",
    "        b_probs = b_probs / torch.sum(b_probs)\n",
    "        b = torch.distributions.Categorical(probs=b_probs)\n",
    "\n",
    "        action = b.sample()\n",
    "        \n",
    "        # using a greedy approach\n",
    "        guess_id = torch.argmax(b_probs).item()\n",
    "        \n",
    "        # guess_id = action.item()\n",
    "        guess = ids[guess_id]\n",
    "        \n",
    "        next_state, r, done, _ = env.step(guess)\n",
    "        \n",
    "        state = [''.join(next_state)]\n",
    "#         print(state) #, guess, r, next_state)\n",
    "        \n",
    "        left[0, guess_id] = 0.\n",
    "        \n",
    "        cr += r\n",
    "        # print(guess, cr)\n",
    "    \n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn.transformer.encoder.embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from env.hangman import Hangman, HangmanEnv\n",
    "\n",
    "def test_pgn2(valloader):\n",
    "    \n",
    "    wins = 0\n",
    "    reward = 0\n",
    "    total_games = 0\n",
    "    pgn.eval()\n",
    "    for i, state in enumerate(valloader):\n",
    "        \n",
    "        if total_games > 10: return\n",
    "        \n",
    "        cr = mini_sim2(state)\n",
    "        if cr > - 6:\n",
    "            wins += 1\n",
    "            # print(state)\n",
    "        total_games += 1\n",
    "        reward += cr\n",
    "        \n",
    "        avg_reward = reward / total_games\n",
    "        win_rate = wins / total_games\n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_ = time.time()\n",
    "test_pgn2(valloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "trexquant_challenge.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
