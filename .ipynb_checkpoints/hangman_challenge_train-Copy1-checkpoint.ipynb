{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V6hm8XxxZFSa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.Models import Transformer, Transformer2\n",
    "from model.Optim import CosineWithRestarts\n",
    "from model.Batch import create_masks\n",
    "from utils.utils import MyTokenizer, MyMasker\n",
    "from utils.data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Okn0Xe3LCp6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225027 2273\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "bs=128\n",
    "dataset = TextDataset()\n",
    "train_size = int(0.99*len(dataset))\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ed2KNafObWfJ"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(32)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MtIO-GZjZVxX"
   },
   "outputs": [],
   "source": [
    "# Loading Tranformer model from scratch\n",
    "max_len = 32\n",
    "model = Transformer(src_vocab=28, d_model=128, max_seq_len=max_len, N=12, heads=8, dropout=0.1)\n",
    "model.to('cuda')\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Lut9a2Jyk2Q"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(max_len)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rt2HRx-NZMZl"
   },
   "outputs": [],
   "source": [
    "def train_model(model, bs, epochs, printevery):\n",
    "\n",
    "    print(\"training model...\")\n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        print('gpu detected!')\n",
    "    else:\n",
    "        print('no gpu detected')\n",
    "        return 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, trg in enumerate(trainloader):\n",
    "\n",
    "            # src = batch.src.transpose(0,1)\n",
    "            # trg = batch.trg.transpose(0,1)\n",
    "            # trg_input = trg[:, :-1]\n",
    "            # src_mask, _ = create_masks(src, trg_input) # need to edit\n",
    "\n",
    "            # test to check if overfit\n",
    "\n",
    "            # src is the incomplete word\n",
    "            perc=None\n",
    "            src = masker.mask(trg, perc)  # e.g. [m_zh__n, _s, _w_eso_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            trg = tokenizer.encode(trg)\n",
    "\n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "\n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                trg = trg.to('cuda')\n",
    "            \n",
    "            model.train()\n",
    "            # preds = model(src, mask)\n",
    "            preds = model(src)\n",
    "            # ys = trg[:, 1:].contiguous().view(-1)\n",
    "            # y = mask.squeeze(1)\n",
    "            \n",
    "            # \n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), trg.contiguous().view(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print(i+1)\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(trainloader.dataset) * bs)\n",
    "                avg_loss = total_loss / printevery\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='')\n",
    "                total_loss = 0\n",
    "\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), f'./weights/model_automask_weights_{datetime.today().strftime(\"%m%d%Y\")}')\n",
    "                pass\n",
    "                \n",
    "        total_val_loss = 0\n",
    "        sims = 0\n",
    "        for i, val in enumerate(valloader):\n",
    "            perc=None\n",
    "            src = masker.mask(val, perc)  # e.g. [m_zh__n, _s, _w_eso_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            val = tokenizer.encode(val)\n",
    "            \n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "            \n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                val = val.to('cuda')\n",
    "            \n",
    "            model.eval()\n",
    "            preds = model(src)\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), val.contiguous().view(-1), ignore_index=0)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            sims += 1\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(valloader.dataset) * bs)\n",
    "                avg_val_loss = total_val_loss / sims\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_val_loss), end='')\n",
    "            \n",
    "        print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, val loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RZ1vQwRQa5tV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_model(model, bs=bs, epochs=25, printevery=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = ord('a')\n",
    "alphabets = {'_': 27}\n",
    "ids = {27:'_', 0:''}\n",
    "for i in range(26):\n",
    "    ch = chr(start)\n",
    "    alphabets[ch] = i+1\n",
    "    ids[i+1] = ch\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGN(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (embed): Embedder(\n",
       "        (embed): Embedding(28, 128, padding_idx=0)\n",
       "      )\n",
       "      (pe): PositionalEncoder(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x EncoderLayer(\n",
       "          (norm_1): Norm()\n",
       "          (norm_2): Norm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (linear_1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Norm()\n",
       "    )\n",
       "    (out): Linear(in_features=128, out_features=28, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from model.Models import PGN\n",
    "\n",
    "\n",
    "pgn = PGN(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "pgn.transformer.load_state_dict(torch.load('./weights/model_weights_03202024'))\n",
    "\n",
    "'''\n",
    "pgn = PGN(src_vocab=28, d_model=32, max_seq_len=32, N=2, heads=4, dropout=0.1)\n",
    "pgn.transformer.load_state_dict(torch.load('./weights/model_weights_lite_1'))\n",
    "'''\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pgn.to('cuda')\n",
    "\n",
    "pgn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim(envs):\n",
    "    obs, info = envs.reset()\n",
    "    state = info['word']\n",
    "    \n",
    "    mask, _ = create_masks(torch.tensor(obs))\n",
    "    mask = mask.to('cuda')\n",
    "    \n",
    "    left = torch.ones((1, 28)).to('cuda')\n",
    "    left[0,  0] = 0.\n",
    "    left[0, -1] = 0.\n",
    "    \n",
    "    done = False\n",
    "    cr = 0\n",
    "    while not done:       \n",
    "        state = tokenizer.encode(state)\n",
    "        state = state.to('cuda')\n",
    "        \n",
    "        probs = pgn(state, mask)\n",
    "        \n",
    "        b_probs = torch.mul(probs, left)\n",
    "        b_probs = b_probs / torch.sum(b_probs)\n",
    "        print(b_probs)\n",
    "        b = torch.distributions.Categorical(probs=b_probs)\n",
    "\n",
    "        action = b.sample()\n",
    "        \n",
    "        # using a greedy approach\n",
    "        guess_id = torch.argmax(b_probs).item()\n",
    "        \n",
    "        # guess_id = action.item()\n",
    "        guess = ids[guess_id]\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = envs.step(guess)\n",
    "        state = info['word']\n",
    "\n",
    "        \n",
    "        left[0, guess_id] = 0.\n",
    "        \n",
    "        cr += reward\n",
    "        # print(guess, cr)\n",
    "    \n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.hangman import Hangman, HangmanEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "def test_pgn(valloader):\n",
    "    \n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda: HangmanEnv(valloader) for i in range(valloader.batch_size)]\n",
    "    )\n",
    "    \n",
    "    wins = 0\n",
    "    reward = 0\n",
    "    total_games = 0\n",
    "    pgn.eval()\n",
    "    for i, state in enumerate(valloader):\n",
    "        \n",
    "        if total_games > 10 : return\n",
    "        \n",
    "        cr = mini_sim(envs)\n",
    "        if cr > - 6:\n",
    "            wins += 1\n",
    "            # print(state)\n",
    "        total_games += 1\n",
    "        reward += cr\n",
    "        \n",
    "        avg_reward = reward / total_games\n",
    "        win_rate = wins / total_games\n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0882, 0.0236, 0.0341, 0.0349, 0.1156, 0.0171, 0.0269, 0.0273,\n",
      "         0.0818, 0.0022, 0.0160, 0.0640, 0.0275, 0.0645, 0.0615, 0.0277, 0.0016,\n",
      "         0.0695, 0.0810, 0.0566, 0.0331, 0.0093, 0.0145, 0.0026, 0.0150, 0.0040,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0997, 0.0267, 0.0385, 0.0394, 0.0000, 0.0193, 0.0304, 0.0309,\n",
      "         0.0925, 0.0025, 0.0181, 0.0723, 0.0311, 0.0729, 0.0696, 0.0314, 0.0018,\n",
      "         0.0785, 0.0916, 0.0640, 0.0375, 0.0105, 0.0163, 0.0030, 0.0170, 0.0045,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0209, 0.0503, 0.0365, 0.0000, 0.0131, 0.0332, 0.0245,\n",
      "         0.1198, 0.0011, 0.0078, 0.0780, 0.0337, 0.0858, 0.0888, 0.0374, 0.0017,\n",
      "         0.0738, 0.1160, 0.0840, 0.0485, 0.0099, 0.0056, 0.0044, 0.0201, 0.0052,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0173, 0.1172, 0.0432, 0.0000, 0.0174, 0.0282, 0.0230,\n",
      "         0.0000, 0.0003, 0.0065, 0.0983, 0.0236, 0.1386, 0.0477, 0.0447, 0.0028,\n",
      "         0.0520, 0.1670, 0.1382, 0.0140, 0.0077, 0.0012, 0.0059, 0.0020, 0.0030,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 1.7910e-02, 1.7828e-01, 3.6339e-02, 0.0000e+00,\n",
      "         2.2886e-02, 2.9613e-02, 3.5162e-02, 0.0000e+00, 2.4930e-04, 7.8925e-03,\n",
      "         5.6667e-02, 4.7298e-02, 9.5019e-02, 1.0611e-02, 8.2446e-02, 2.4450e-03,\n",
      "         5.0154e-02, 0.0000e+00, 2.9783e-01, 1.4914e-02, 7.0750e-03, 1.9906e-03,\n",
      "         3.2296e-03, 9.9627e-04, 9.9212e-04, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 1.7182e-03, 2.3617e-01, 1.7929e-02, 0.0000e+00,\n",
      "         7.1034e-03, 2.8636e-03, 9.3681e-02, 0.0000e+00, 5.1236e-05, 1.2008e-02,\n",
      "         2.1692e-02, 1.5006e-01, 3.3088e-01, 1.4636e-02, 8.3754e-02, 1.7635e-04,\n",
      "         1.6138e-02, 0.0000e+00, 0.0000e+00, 5.2543e-03, 1.3454e-03, 4.9191e-04,\n",
      "         3.3142e-03, 3.9223e-04, 3.3121e-04, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 1.9915e-03, 3.9955e-01, 2.8779e-02, 0.0000e+00,\n",
      "         4.9162e-03, 1.8248e-03, 1.0349e-01, 0.0000e+00, 7.0651e-05, 1.5314e-02,\n",
      "         3.6934e-02, 2.5733e-01, 0.0000e+00, 2.0393e-02, 1.0868e-01, 2.4662e-04,\n",
      "         1.0067e-02, 0.0000e+00, 0.0000e+00, 1.8654e-03, 1.8248e-03, 4.9254e-04,\n",
      "         5.0578e-03, 7.2372e-04, 4.4634e-04, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 3.3167e-03, 0.0000e+00, 4.7928e-02, 0.0000e+00,\n",
      "         8.1875e-03, 3.0391e-03, 1.7236e-01, 0.0000e+00, 1.1766e-04, 2.5504e-02,\n",
      "         6.1511e-02, 4.2856e-01, 0.0000e+00, 3.3963e-02, 1.8100e-01, 4.1073e-04,\n",
      "         1.6766e-02, 0.0000e+00, 0.0000e+00, 3.1066e-03, 3.0391e-03, 8.2028e-04,\n",
      "         8.4234e-03, 1.2053e-03, 7.4335e-04, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 5.8041e-03, 0.0000e+00, 8.3873e-02, 0.0000e+00,\n",
      "         1.4328e-02, 5.3183e-03, 3.0163e-01, 0.0000e+00, 2.0591e-04, 4.4632e-02,\n",
      "         1.0764e-01, 0.0000e+00, 0.0000e+00, 5.9433e-02, 3.1674e-01, 7.1876e-04,\n",
      "         2.9339e-02, 0.0000e+00, 0.0000e+00, 5.4365e-03, 5.3183e-03, 1.4355e-03,\n",
      "         1.4741e-02, 2.1092e-03, 1.3008e-03, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 2.7792e-05, 0.0000e+00, 3.3870e-04, 0.0000e+00,\n",
      "         6.4539e-05, 9.6459e-05, 9.3814e-01, 0.0000e+00, 2.7022e-05, 1.0153e-02,\n",
      "         4.8555e-02, 0.0000e+00, 0.0000e+00, 1.7133e-04, 0.0000e+00, 1.9266e-06,\n",
      "         1.2086e-03, 0.0000e+00, 0.0000e+00, 1.4041e-04, 2.7762e-05, 8.7415e-04,\n",
      "         3.2620e-05, 5.5957e-05, 8.0310e-05, 0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0596, 0.0000, 0.0881, 0.0000, 0.0431, 0.0680, 0.0000,\n",
      "         0.0000, 0.0056, 0.0403, 0.1616, 0.0000, 0.0000, 0.1554, 0.0000, 0.0041,\n",
      "         0.1756, 0.0000, 0.0000, 0.0837, 0.0235, 0.0365, 0.0067, 0.0380, 0.0101,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0723, 0.0000, 0.1069, 0.0000, 0.0523, 0.0825, 0.0000,\n",
      "         0.0000, 0.0068, 0.0489, 0.1960, 0.0000, 0.0000, 0.1885, 0.0000, 0.0050,\n",
      "         0.0000, 0.0000, 0.0000, 0.1015, 0.0285, 0.0443, 0.0081, 0.0461, 0.0122,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0899, 0.0000, 0.1329, 0.0000, 0.0650, 0.1026, 0.0000,\n",
      "         0.0000, 0.0085, 0.0609, 0.0000, 0.0000, 0.0000, 0.2345, 0.0000, 0.0062,\n",
      "         0.0000, 0.0000, 0.0000, 0.1263, 0.0355, 0.0551, 0.0101, 0.0573, 0.0152,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.1174, 0.0000, 0.1737, 0.0000, 0.0849, 0.1341, 0.0000,\n",
      "         0.0000, 0.0111, 0.0795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0081,\n",
      "         0.0000, 0.0000, 0.0000, 0.1650, 0.0464, 0.0720, 0.0132, 0.0749, 0.0198,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.1421, 0.0000, 0.0000, 0.0000, 0.1028, 0.1623, 0.0000,\n",
      "         0.0000, 0.0134, 0.0962, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0098,\n",
      "         0.0000, 0.0000, 0.0000, 0.1997, 0.0561, 0.0871, 0.0159, 0.0906, 0.0240,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.1776, 0.0000, 0.0000, 0.0000, 0.1284, 0.2028, 0.0000,\n",
      "         0.0000, 0.0167, 0.1202, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0123,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0701, 0.1088, 0.0199, 0.1132, 0.0300,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.2228, 0.0000, 0.0000, 0.0000, 0.1611, 0.0000, 0.0000,\n",
      "         0.0000, 0.0210, 0.1508, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0154,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0879, 0.1365, 0.0249, 0.1420, 0.0376,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2072, 0.0000, 0.0000,\n",
      "         0.0000, 0.0270, 0.1940, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0198,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1131, 0.1756, 0.0321, 0.1827, 0.0483,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0341, 0.2448, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0250,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1427, 0.2216, 0.0405, 0.2305, 0.0610,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0330,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1889, 0.2934, 0.0536, 0.3052, 0.0807,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2719, 0.4222, 0.0772, 0.0000, 0.1162,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1124, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0823,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4706, 0.0000, 0.1335, 0.0000, 0.2011,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1555,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2523, 0.0000, 0.3799,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3424, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2507,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4068, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4227,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 28)) of distribution Categorical(probs: torch.Size([1, 28])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m t_ \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 2\u001b[0m test_pgn(valloader)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_)\n",
      "Cell \u001b[1;32mIn[66], line 18\u001b[0m, in \u001b[0;36mtest_pgn\u001b[1;34m(valloader)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valloader):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_games \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m : \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     cr \u001b[38;5;241m=\u001b[39m mini_sim(envs)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m6\u001b[39m:\n\u001b[0;32m     20\u001b[0m         wins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[65], line 23\u001b[0m, in \u001b[0;36mmini_sim\u001b[1;34m(envs)\u001b[0m\n\u001b[0;32m     21\u001b[0m b_probs \u001b[38;5;241m=\u001b[39m b_probs \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(b_probs)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(b_probs)\n\u001b[1;32m---> 23\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs\u001b[38;5;241m=\u001b[39mb_probs)\n\u001b[0;32m     25\u001b[0m action \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# using a greedy approach\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rlenv\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rlenv\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 28)) of distribution Categorical(probs: torch.Size([1, 28])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n         nan, nan, nan, nan]], device='cuda:0', grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "t_ = time.time()\n",
    "test_pgn(valloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'collocationable'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valloader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(idx):\n",
    "    def thunk():\n",
    "        env = HangmanEnv(dataloader=valloader)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda: HangmanEnv(valloader) for i in range(valloader.batch_size)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': array(['_________________', '___________', '_____________'], dtype=object), '_word': array([ True,  True,  True])}\n",
      "['_________________', '_______a___', '_____________']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "obs, info = envs.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = ['ab', 'a', 'a']\n",
    "    obs, reward, terminated, truncated, info = envs.step(action)\n",
    "    print(tokenizer.decode(obs))\n",
    "    break\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim2(sample):\n",
    "    env = HangmanEnv(sample[0])\n",
    "    n = len(sample[0])\n",
    "    state = masker.mask(sample, 1)\n",
    "    sample_mask, _ = create_masks(tokenizer.encode(sample))\n",
    "    mask = sample_mask.to('cuda')\n",
    "    y = sample_mask.squeeze(1).to('cuda')\n",
    "    y_float = torch.where(y, 1., 0.)\n",
    "    \n",
    "    left = torch.ones((1, 28)).to('cuda')\n",
    "    left[0,  0] = 0.\n",
    "    left[0, -1] = 0.\n",
    "    \n",
    "    P = nn.Softmax(dim=-1)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    cr = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # print(state)\n",
    "        \n",
    "        state = tokenizer.encode(state)\n",
    "        state = state.to('cuda')\n",
    "        \n",
    "        # q_probs = score / torch.sum(score)\n",
    "        \n",
    "        probs = pgn(state, mask)\n",
    "        \n",
    "        b_probs = torch.mul(probs, left)\n",
    "        b_probs = b_probs / torch.sum(b_probs)\n",
    "        b = torch.distributions.Categorical(probs=b_probs)\n",
    "\n",
    "        action = b.sample()\n",
    "        \n",
    "        # using a greedy approach\n",
    "        guess_id = torch.argmax(b_probs).item()\n",
    "        \n",
    "        # guess_id = action.item()\n",
    "        guess = ids[guess_id]\n",
    "        \n",
    "        next_state, r, done, _ = env.step(guess)\n",
    "        \n",
    "        state = [''.join(next_state)]\n",
    "#         print(state) #, guess, r, next_state)\n",
    "        \n",
    "        left[0, guess_id] = 0.\n",
    "        \n",
    "        cr += r\n",
    "        # print(guess, cr)\n",
    "    \n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from env.hangman import Hangman, HangmanEnv\n",
    "\n",
    "def test_pgn2(valloader):\n",
    "    \n",
    "    wins = 0\n",
    "    reward = 0\n",
    "    total_games = 0\n",
    "    pgn.eval()\n",
    "    for i, state in enumerate(valloader):\n",
    "        \n",
    "        if total_games > 10: return\n",
    "        \n",
    "        cr = mini_sim2(state)\n",
    "        if cr > - 6:\n",
    "            wins += 1\n",
    "            # print(state)\n",
    "        total_games += 1\n",
    "        reward += cr\n",
    "        \n",
    "        avg_reward = reward / total_games\n",
    "        win_rate = wins / total_games\n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wins : 6 \t total games : 11 \t win rate : 54.545% \t reward : -3.000 \t average reward : -5.364 \n",
      " 1.878509521484375\n"
     ]
    }
   ],
   "source": [
    "t_ = time.time()\n",
    "test_pgn2(valloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "trexquant_challenge.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
