{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V6hm8XxxZFSa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.Models import Transformer, Transformer2\n",
    "from model.Optim import CosineWithRestarts\n",
    "from model.Batch import create_masks\n",
    "from utils.utils import MyTokenizer, MyMasker\n",
    "from utils.data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Okn0Xe3LCp6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225027 2273\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "bs=128\n",
    "dataset = TextDataset()\n",
    "train_size = int(0.99*len(dataset))\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ed2KNafObWfJ"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dataloader, max_seq_len=32, init_counter=0):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        self.dataset = shuffle(dataloader.dataset)\n",
    "        self.counter = init_counter\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.action_space = spaces.Discrete(28)  # 26 possible actions (a-z) + '' + '_'\n",
    "        self.observation_space = spaces.Box(low=0, high=27, shape=(self.max_seq_len,), dtype=int)\n",
    "\n",
    "        self.hidden_word = None\n",
    "        self.word_length = None\n",
    "        self._reset_attributes()\n",
    "    \n",
    "    def _reset_attributes(self):\n",
    "        self.guessed_letters = set()\n",
    "        self.remaining_attempts = 6  # Maximum attempts\n",
    "        self.current_state = np.zeros(self.max_seq_len, dtype=int)  # Initial state\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self, *, seed=0, options=None):\n",
    "        self.hidden_word = self.dataset[self.counter % len(self.dataset)]\n",
    "        self.word_length = len(self.hidden_word)\n",
    "        self._reset_attributes()\n",
    "        \n",
    "        # Increment reset counter\n",
    "        self.counter += 1\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def generate_random_word(self):\n",
    "        # Replace this with your logic for generating random words\n",
    "        word_list = self.dataset\n",
    "        idx = self.counter % len(word_list)\n",
    "        self.counter += 1\n",
    "        return word_list[idx]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in self.guessed_letters:\n",
    "            print(\"You have already guessed that letter.\")\n",
    "        else:\n",
    "            self.guessed_letters.add(action)\n",
    "            if action in self.hidden_word:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 0\n",
    "                self.remaining_attempts -= 1\n",
    "\n",
    "        if set(self.hidden_word) <= self.guessed_letters or self.remaining_attempts == 0:\n",
    "            reward = 1 if set(self.hidden_word) <= self.guessed_letters else 0\n",
    "            self.game_over = True\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, reward, self.game_over, self.game_over, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def word2state(self, word):\n",
    "        state = [27 if char == '_' else ord(char) - ord('a') + 1 for char in word]\n",
    "        while len(state) < self.max_seq_len:\n",
    "            state.append(0)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(guessed_letters):\n",
    "    \n",
    "    valid_actions = torch.ones((len(guessed_letters), 28)).to('cuda')\n",
    "    valid_actions[:,  0] = 0.\n",
    "    valid_actions[:, -1] = 0.\n",
    "    \n",
    "    for i, s in enumerate(guessed_letters):\n",
    "        for char in s:\n",
    "            idx = ord(char) - ord('a') + 1\n",
    "            valid_actions[i, idx] = 0.\n",
    "    \n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, probs=None, logits=None, validate_args=None, masks=[]):\n",
    "        \n",
    "        self.masks = masks\n",
    "        if len(self.masks) == 0:\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "        else:\n",
    "            self.masks = masks.type(torch.BoolTensor).to(device)\n",
    "            logits = torch.where(self.masks, logits, torch.tensor(-1e8).to(device))\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "\n",
    "    def entropy(self):\n",
    "        if len(self.masks) == 0:\n",
    "            return super(CategoricalMasked, self).entropy()\n",
    "        p_log_p = self.logits * self.probs\n",
    "        p_log_p = torch.where(self.masks, p_log_p, torch.tensor(0.0).to(device))\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, temperature=1.):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.pretrainedLLM = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrainedLLM.load_state_dict(torch.load('./weights/model_weights_epoch750_04042024'))\n",
    "        \n",
    "        # helper function\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Flatten output logits from transformer and feed into feed-forward NN\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 28), std=0.01),\n",
    "        )\n",
    "        \n",
    "        # temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def get_value(self, x):\n",
    "#         with torch.no_grad():\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "        probs_masked_flatten = probs_masked.view((probs_masked.shape[0], -1))\n",
    "            \n",
    "        return self.critic(probs_masked_flatten.detach())\n",
    "\n",
    "    def get_action_and_value(self, x, valid_actions, action=None):\n",
    "#         with torch.no_grad():\n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x) / self.temperature\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "        probs_masked_flatten = probs_masked.view((probs_masked.shape[0], -1))\n",
    "\n",
    "        probs = torch.matmul(1.*mask, probs)  # effectively adds the probs row-wise for each action / character\n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs / torch.sum(probs)\n",
    "\n",
    "        fprobs = torch.mul(probs, valid_actions)\n",
    "        fprobs = fprobs / torch.sum(fprobs)\n",
    "\n",
    "        # Choose max probable actions as int form\n",
    "        action = torch.argmax(fprobs, dim=-1)\n",
    "        logprob = torch.max(fprobs, dim=-1)[0].log()\n",
    "        \n",
    "        return action, logprob, fprobs * 0., self.critic(probs_masked_flatten.detach()) * 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim(agent, envs, optimizer, train=False):\n",
    "    \n",
    "    n = envs.num_envs\n",
    "    horizon = 32\n",
    "    batch_size = horizon*n\n",
    "    minibatch_size = batch_size // 4\n",
    "    num_updates = 10000000 // batch_size\n",
    "    \n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Initializing simulation matrices for the given batched episode\n",
    "    observations = torch.zeros((horizon, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "    actions = torch.zeros((horizon, n)).to(device)\n",
    "    logprobs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "    \n",
    "    \n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_t = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, info = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros((n,)).to(device)\n",
    "    \n",
    "    valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "    \n",
    "    done = False\n",
    "    cr = 0.\n",
    "    \n",
    "    wins = 0\n",
    "    total_games = 0\n",
    "    while True:\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            global_t += 1 * n\n",
    "            observations[t] = next_obs\n",
    "            dones[t] = next_done\n",
    "\n",
    "            action_ints, logprob, _, _ = agent.get_action_and_value(next_obs, valid_actions)\n",
    "            actions[t] = action_ints\n",
    "            logprobs[t] = logprob\n",
    "        \n",
    "            # Convert to action_int to action_str guesses\n",
    "            action_strs =  [chr(idx-1 + ord('a')) for idx in action_ints]\n",
    "            \n",
    "            \n",
    "            # Take step in the envs\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action_strs)\n",
    "            done = (terminated | truncated)\n",
    "            \n",
    "            # log data\n",
    "            rewards[t] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).to(device)\n",
    "            next_done = torch.tensor(done).to(device)\n",
    "            \n",
    "            valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "        \n",
    "        wins += rewards.sum()\n",
    "        total_games += dones.sum()\n",
    "        \n",
    "        win_rate = wins / total_games\n",
    "        curr_win_rate = rewards.sum() / dones.sum()\n",
    "        \n",
    "        mean_time_per_game = (time.time() - start_time) / total_games \n",
    "        \n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t curr win rate : %.03f%% \\t time_per_game : %.03f ms' \\\n",
    "              %(wins, total_games, 100*win_rate, 100*curr_win_rate, 1000*mean_time_per_game), end='')\n",
    "        \n",
    "        if train:\n",
    "            gamma = 1.\n",
    "            gae_lambda = 0.95\n",
    "            clip_coef = 0.1\n",
    "            ent_coef = 0.01\n",
    "            vf_coef = 0.5\n",
    "            max_grad_norm = 0.5\n",
    "\n",
    "            # bootstrap value if not done\n",
    "            with torch.no_grad():\n",
    "                next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                if False: # if args.gae:\n",
    "                    advantages = torch.zeros_like(rewards).to(device)\n",
    "                    lastgaelam = 0\n",
    "                    for t in reversed(range(horizon)):\n",
    "                        if t == horizon - 1:\n",
    "                            nextnonterminal = 1.0 - 1.*next_done\n",
    "                            nextvalues = next_value\n",
    "                        else:\n",
    "                            nextnonterminal = 1.0 - 1.*dones[t + 1]\n",
    "                            nextvalues = values[t + 1]\n",
    "                        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                    returns = advantages + values\n",
    "                else:\n",
    "                    returns = torch.zeros_like(rewards).to(device)\n",
    "                    for t in reversed(range(horizon)):\n",
    "                        if t == horizon - 1:\n",
    "                            nextnonterminal = 1.0 - 1.*next_done\n",
    "                            next_return = 0. # next_value\n",
    "                        else:\n",
    "                            nextnonterminal = 1.0 - 1.*dones[t + 1]\n",
    "                            next_return = returns[t + 1]\n",
    "                        returns[t] = rewards[t] + gamma * nextnonterminal * next_return\n",
    "                    advantages = returns #- values\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            loss = (logprobs * returns).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Initializing simulation matrices for the given batched episode\n",
    "        observations = torch.zeros((horizon, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "        actions = torch.zeros((horizon, n)).to(device)\n",
    "        logprobs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "        rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "        dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "            \n",
    "\n",
    "        \n",
    "    envs.close()\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wins : 7339 \t total games : 11079 \t win rate : 66.242% \t curr win rate : 78.000% \t time_per_game : 72.974 ms"
     ]
    }
   ],
   "source": [
    "# from env.hangman import HangmanEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "# agent = Agent(envs).to(device)\n",
    "# optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-5, eps=1e-5)\n",
    "\n",
    "# def test_pgn(dataloader):\n",
    "dataloader = trainloader\n",
    "    \n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: HangmanEnv(dataloader) for i in range(dataloader.batch_size)]\n",
    ")\n",
    "\n",
    "agent = Agent(envs, temperature=1.).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "cr = mini_sim(agent, envs, optimizer, train=True)\n",
    "\n",
    "#         print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "trexquant_challenge.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
