{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V6hm8XxxZFSa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.Models import Transformer, Transformer2\n",
    "from model.Optim import CosineWithRestarts\n",
    "from model.Batch import create_masks\n",
    "from utils.utils import MyTokenizer, MyMasker\n",
    "from utils.data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Okn0Xe3LCp6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225027 2273\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "bs=128\n",
    "dataset = TextDataset()\n",
    "train_size = int(0.99*len(dataset))\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ed2KNafObWfJ"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(32)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MtIO-GZjZVxX"
   },
   "outputs": [],
   "source": [
    "# Loading Tranformer model from scratch\n",
    "max_len = 32\n",
    "model = Transformer(src_vocab=28, d_model=128, max_seq_len=max_len, N=12, heads=8, dropout=0.1)\n",
    "model.to('cuda')\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Lut9a2Jyk2Q"
   },
   "outputs": [],
   "source": [
    "masker = MyMasker()\n",
    "tokenizer = MyTokenizer(max_len)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rt2HRx-NZMZl"
   },
   "outputs": [],
   "source": [
    "def train_model(model, bs, epochs, printevery):\n",
    "\n",
    "    print(\"training model...\")\n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        print('gpu detected!')\n",
    "    else:\n",
    "        print('no gpu detected')\n",
    "        return 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, trg in enumerate(trainloader):\n",
    "\n",
    "            # src = batch.src.transpose(0,1)\n",
    "            # trg = batch.trg.transpose(0,1)\n",
    "            # trg_input = trg[:, :-1]\n",
    "            # src_mask, _ = create_masks(src, trg_input) # need to edit\n",
    "\n",
    "            # test to check if overfit\n",
    "\n",
    "            # src is the incomplete word\n",
    "            perc=None\n",
    "            src = masker.mask(trg, perc)  # e.g. [m_zh__n, _s, _w_eso_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            trg = tokenizer.encode(trg)\n",
    "\n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "\n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                trg = trg.to('cuda')\n",
    "            \n",
    "            model.train()\n",
    "            # preds = model(src, mask)\n",
    "            preds = model(src)\n",
    "            # ys = trg[:, 1:].contiguous().view(-1)\n",
    "            # y = mask.squeeze(1)\n",
    "            \n",
    "            # \n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), trg.contiguous().view(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print(i+1)\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(trainloader.dataset) * bs)\n",
    "                avg_loss = total_loss / printevery\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='')\n",
    "                total_loss = 0\n",
    "\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                torch.save(model.state_dict(), f'./weights/model_automask_weights_{datetime.today().strftime(\"%m%d%Y\")}')\n",
    "                pass\n",
    "                \n",
    "        total_val_loss = 0\n",
    "        sims = 0\n",
    "        for i, val in enumerate(valloader):\n",
    "            perc=None\n",
    "            src = masker.mask(val, perc)  # e.g. [m_zh__n, _s, _w_eso_e]\n",
    "            src = tokenizer.encode(src)  # e.g. [[], [], []]\n",
    "            \n",
    "            # trg is the complete word\n",
    "            val = tokenizer.encode(val)\n",
    "            \n",
    "            # our src_mask is the same as trg_mask = mask\n",
    "            mask, _ = create_masks(src)  # e.g. [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 0]]\n",
    "            \n",
    "            # Converting to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                src = src.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "                val = val.to('cuda')\n",
    "            \n",
    "            model.eval()\n",
    "            preds = model(src)\n",
    "            \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), val.contiguous().view(-1), ignore_index=0)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            sims += 1\n",
    "            if (i + 1) % printevery == 0:\n",
    "                p = int(100 * (i + 1) / len(valloader.dataset) * bs)\n",
    "                avg_val_loss = total_val_loss / sims\n",
    "                print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_val_loss), end='')\n",
    "            \n",
    "        print(\"\\r   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, val loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RZ1vQwRQa5tV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_model(model, bs=bs, epochs=25, printevery=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = ord('a')\n",
    "alphabets = {'_': 27}\n",
    "ids = {27:'_', 0:''}\n",
    "for i in range(26):\n",
    "    ch = chr(start)\n",
    "    alphabets[ch] = i+1\n",
    "    ids[i+1] = ch\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGN(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (embed): Embedder(\n",
       "        (embed): Embedding(28, 128, padding_idx=0)\n",
       "      )\n",
       "      (pe): PositionalEncoder(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x EncoderLayer(\n",
       "          (norm_1): Norm()\n",
       "          (norm_2): Norm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (linear_1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Norm()\n",
       "    )\n",
       "    (out): Linear(in_features=128, out_features=28, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from model.Models import PGN\n",
    "\n",
    "\n",
    "pgn = PGN(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "pgn.transformer.load_state_dict(torch.load('./weights/model_weights_03202024'))\n",
    "\n",
    "'''\n",
    "pgn = PGN(src_vocab=28, d_model=32, max_seq_len=32, N=2, heads=4, dropout=0.1)\n",
    "pgn.transformer.load_state_dict(torch.load('./weights/model_weights_lite_1'))\n",
    "'''\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pgn.to('cuda')\n",
    "\n",
    "pgn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trajectories(envs, policy, horizon, device):\n",
    "    n = envs.num_envs\n",
    "\n",
    "    # Initializing simulation matrices for the given batched episode\n",
    "    log_probs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "\n",
    "    obs, _ = envs.reset()\n",
    "    done = np.zeros((n,), dtype=bool)  # e.g. [False, False, False]\n",
    "    T = None\n",
    "\n",
    "    for t in range(horizon):\n",
    "        obs = torch.tensor(np.float32(obs)).to(device)\n",
    "\n",
    "        action, log_prob = policy.get_action(obs)\n",
    "\n",
    "        log_probs[t] = log_prob\n",
    "        dones[t] = torch.tensor(done).to(device)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = envs.step(action.cpu().detach().numpy())\n",
    "        done = done | (np.array(terminated) | np.array(truncated))\n",
    "\n",
    "        # Modify rewards to NOT consider data points after `done`\n",
    "        reward = reward * ~done\n",
    "        rewards[t] = torch.tensor(reward).to(device)\n",
    "\n",
    "        if done.all():\n",
    "            T = t\n",
    "            break\n",
    "\n",
    "    cum_discounted_rewards = discount_cumsum(rewards, dones, gamma=0.99, normalize=False, device=device)\n",
    "    mean_episode_return = torch.sum(cum_discounted_rewards, axis=0) / torch.sum(~dones, axis=0)\n",
    "\n",
    "    traj_info = {\n",
    "        'log_probs': log_probs[:T],\n",
    "        'rewards': rewards[:T],\n",
    "        'dones': dones[:T],\n",
    "    }\n",
    "\n",
    "    return traj_info, torch.sum(rewards, axis=0), mean_episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((3, 28, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dataloader, max_seq_len=32, init_counter=0):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        self.dataset = shuffle(dataloader.dataset)\n",
    "        self.counter = init_counter\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.action_space = spaces.Discrete(28)  # 26 possible actions (a-z) + '' + '_'\n",
    "        self.observation_space = spaces.Box(low=0, high=27, shape=(self.max_seq_len,), dtype=int)\n",
    "\n",
    "        self.hidden_word = None\n",
    "        self.word_length = None\n",
    "        self._reset_attributes()\n",
    "    \n",
    "    def _reset_attributes(self):\n",
    "        self.guessed_letters = set()\n",
    "        self.remaining_attempts = 6  # Maximum attempts\n",
    "        self.current_state = np.zeros(self.max_seq_len, dtype=int)  # Initial state\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self, *, seed=0, options=None):\n",
    "        self.hidden_word = self.dataset[self.counter]\n",
    "        self.word_length = len(self.hidden_word)\n",
    "        self._reset_attributes()\n",
    "        \n",
    "        # Increment reset counter\n",
    "        self.counter += 1\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def generate_random_word(self):\n",
    "        # Replace this with your logic for generating random words\n",
    "        word_list = self.dataset\n",
    "        idx = self.counter % len(word_list)\n",
    "        self.counter += 1\n",
    "        return word_list[idx]\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in self.guessed_letters:\n",
    "            print(\"You have already guessed that letter.\")\n",
    "        else:\n",
    "            self.guessed_letters.add(action)\n",
    "            if action in self.hidden_word:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 0\n",
    "                self.remaining_attempts -= 1\n",
    "\n",
    "        if set(self.hidden_word) <= self.guessed_letters or self.remaining_attempts == 0:\n",
    "            reward = 1 if set(self.hidden_word) <= self.guessed_letters else 0\n",
    "            self.game_over = True\n",
    "\n",
    "        current_word = ''.join([char if char in self.guessed_letters else '_' for char in self.hidden_word])\n",
    "        self.current_state = self.word2state(current_word)\n",
    "        return self.current_state, reward, self.game_over, self.game_over, {'word': current_word, 'hidden_word': self.hidden_word, 'guessed_letters': self.guessed_letters}\n",
    "\n",
    "    def word2state(self, word):\n",
    "        state = [27 if char == '_' else ord(char) - ord('a') + 1 for char in word]\n",
    "        while len(state) < self.max_seq_len:\n",
    "            state.append(0)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(guessed_letters):\n",
    "    \n",
    "    valid_actions = torch.ones((len(guessed_letters), 28)).to('cuda')\n",
    "    valid_actions[:,  0] = 0.\n",
    "    valid_actions[:, -1] = 0.\n",
    "    \n",
    "    for i, s in enumerate(guessed_letters):\n",
    "        for char in s:\n",
    "            idx = ord(char) - ord('a') + 1\n",
    "            valid_actions[i, idx] = 0.\n",
    "    \n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, probs=None, logits=None, validate_args=None, masks=[]):\n",
    "        \n",
    "        self.masks = masks\n",
    "        if len(self.masks) == 0:\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "        else:\n",
    "            self.masks = masks.type(torch.BoolTensor).to(device)\n",
    "            logits = torch.where(self.masks, logits, torch.tensor(-1e8).to(device))\n",
    "            super(CategoricalMasked, self).__init__(probs, logits, validate_args)\n",
    "\n",
    "    def entropy(self):\n",
    "        if len(self.masks) == 0:\n",
    "            return super(CategoricalMasked, self).entropy()\n",
    "        p_log_p = self.logits * self.probs\n",
    "        p_log_p = torch.where(self.masks, p_log_p, torch.tensor(0.0).to(device))\n",
    "        return -p_log_p.sum(-1)\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.pretrainedLLM = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrainedLLM.load_state_dict(torch.load('./weights/model_weights_epoch750_04042024'))\n",
    "        \n",
    "        # helper function\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Flatten output logits from transformer and feed into feed-forward NN\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(32*28, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 28), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        with torch.no_grad():\n",
    "            mask = (x != 0).unsqueeze(-2)\n",
    "            logits = self.pretrainedLLM(x)\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "            \n",
    "        return self.critic(probs_masked.view((probs_masked.shape[0], -1)))\n",
    "\n",
    "    def get_action_and_value(self, x, valid_actions, action=None):\n",
    "        with torch.no_grad():\n",
    "            mask = (x != 0).unsqueeze(-2)\n",
    "            logits = self.pretrainedLLM(x)\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            probs_masked = probs * torch.stack((mask.squeeze(-2),) * probs.shape[-1], dim=-1)\n",
    "            \n",
    "#             print(logits)\n",
    "#             print(probs)\n",
    "\n",
    "#             q = (1-probs+1e-9).log()\n",
    "#             q = torch.matmul(1.*mask, q)\n",
    "#             q = q.squeeze(1)\n",
    "# #             print('before', torch.isnan(q).any().item())\n",
    "# #             print(q)\n",
    "#             q = torch.mul(q, valid_actions)\n",
    "# #             q = q / torch.sum(q)\n",
    "# #             print(valid_actions*1)\n",
    "# #             print('after', torch.isnan(q).any().item())\n",
    "# #             print(q)\n",
    "# #             print('----------------------')\n",
    "#             action_new = torch.argmin(q, dim=-1)\n",
    "            \n",
    "#             print(probs)\n",
    "#             print(torch.isnan((1-probs).log()).any().item())\n",
    "#             print(torch.min(q, dim=-1))\n",
    "        \n",
    "            probs = torch.matmul(1.*mask, probs)  # effectively adds the probs row-wise for each action / character\n",
    "            probs = probs.squeeze(1)\n",
    "            probs = probs / torch.sum(probs)\n",
    "\n",
    "#         x = self.pretrainedLLM(x)\n",
    "#         temp = nn.functional.softmax(x, dim=-1)\n",
    "#         print(temp)\n",
    "#         x = x.view((x.shape[0], -1))\n",
    "        \n",
    "#         logits = self.actor(x)\n",
    "#         probs = CategoricalMasked(logits=logits, masks=valid_actions)\n",
    "#         if action is None:\n",
    "#             action = probs.sample()\n",
    "#         logprob = probs.log_prob(action)\n",
    "#         entropy = probs.entropy()\n",
    "\n",
    "            fprobs = torch.mul(probs, valid_actions)\n",
    "            fprobs = fprobs / torch.sum(fprobs)\n",
    "\n",
    "            # Choose max probable actions as int form\n",
    "            action = torch.argmax(fprobs, dim=-1)\n",
    "            logprob = torch.max(fprobs, dim=-1)[0].log()\n",
    "            entropy = -torch.where(valid_actions.type(torch.BoolTensor).to(device), fprobs*fprobs.log(), torch.tensor(0.0).to(device))\n",
    "            entropy = entropy.sum(-1)\n",
    "\n",
    "#         print(action.shape, logprob.shape, entropy.shape, self.critic(x).shape)\n",
    "#         print(action.shape, logprob.shape, entropy.shape, self.critic(logits.view((logits.shape[0], -1))).shape)\n",
    "#         print(action)\n",
    "        \n",
    "        return action, logprob, entropy, self.critic(probs_masked.view((probs_masked.shape[0], -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim(agent, envs, optimizer):\n",
    "    \n",
    "    n = envs.num_envs\n",
    "    horizon = 32\n",
    "    batch_size = horizon*n\n",
    "    minibatch_size = batch_size // 4\n",
    "    num_updates = 10000000 // batch_size\n",
    "    \n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Initializing agent\n",
    "#     agent = Agent(envs).to(device)\n",
    "#     optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
    "    \n",
    "    # Initializing simulation matrices for the given batched episode\n",
    "    observations = torch.zeros((horizon, n, *envs.single_observation_space.shape), dtype=int).to(device)\n",
    "    actions = torch.zeros((horizon, n)).to(device)\n",
    "    action_masks = torch.zeros((horizon, n) + (envs.single_action_space.n,)).to(device)\n",
    "    values = torch.ones((horizon, n), dtype=torch.float).to(device)\n",
    "    logprobs = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    rewards = torch.zeros((horizon, n), dtype=torch.float32).to(device)\n",
    "    dones = torch.ones((horizon, n), dtype=bool).to(device)\n",
    "    \n",
    "    \n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_t = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, info = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros((n,)).to(device)\n",
    "    \n",
    "    valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "    \n",
    "    done = False\n",
    "    cr = 0.\n",
    "    \n",
    "    wins = 0\n",
    "    total_games = 0\n",
    "#     for update in range(1, num_updates + 1):\n",
    "    while True:\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            global_t += 1 * n\n",
    "            observations[t] = next_obs\n",
    "            dones[t] = next_done\n",
    "            action_masks[t] = valid_actions\n",
    "            \n",
    "#             # Get action probs\n",
    "#             probs = pgn(next_obs)\n",
    "            \n",
    "#             # Zero out invalid actions\n",
    "#             b_probs = torch.mul(probs, valid_actions)\n",
    "#             b_probs = b_probs / torch.sum(b_probs)\n",
    "        \n",
    "#             # Choose max probable actions as int form\n",
    "#             action_ints = torch.argmax(b_probs, dim=-1)\n",
    "            \n",
    "             # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action_ints, logprob, _, value = agent.get_action_and_value(next_obs, valid_actions)\n",
    "                values[t] = value.flatten()\n",
    "            actions[t] = action_ints\n",
    "            logprobs[t] = logprob\n",
    "        \n",
    "            # Convert to action_int to action_str guesses\n",
    "            action_strs =  [chr(idx-1 + ord('a')) for idx in action_ints]\n",
    "            \n",
    "            \n",
    "            # Take step in the envs\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action_strs)\n",
    "            done = (terminated | truncated)\n",
    "            \n",
    "            # log data\n",
    "            rewards[t] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).to(device)\n",
    "            next_done = torch.tensor(done).to(device)\n",
    "            \n",
    "            valid_actions = get_valid_actions(info['guessed_letters'])\n",
    "        \n",
    "        wins += rewards.sum()\n",
    "        total_games += dones.sum()\n",
    "        \n",
    "        win_rate = wins / total_games\n",
    "        \n",
    "        mean_time_per_game = (time.time() - start_time) / total_games \n",
    "        \n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t time_per_game : %.03f ms \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, 1000*mean_time_per_game, 0), end='')\n",
    "        \n",
    "#         '''\n",
    "        gamma = 0.99\n",
    "        gae_lambda = 0.95\n",
    "        clip_coef = 0.1\n",
    "        ent_coef = 0.01\n",
    "        vf_coef = 0.5\n",
    "        max_grad_norm = 0.5\n",
    "        \n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            if True: # if args.gae:\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(horizon)):\n",
    "                    if t == horizon - 1:\n",
    "                        nextnonterminal = 1.0 - 1.*next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - 1.*dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards).to(device)\n",
    "                for t in reversed(range(horizon)):\n",
    "                    if t == horizon - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        next_return = returns[t + 1]\n",
    "                    returns[t] = rewards[t] + gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "        \n",
    "        # flatten the batch\n",
    "        b_obs = observations.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape(-1)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "        b_action_masks = action_masks.reshape((-1, action_masks.shape[-1]))\n",
    "        \n",
    "        \n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(10):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
    "                    b_obs[mb_inds],\n",
    "                    b_action_masks[mb_inds],\n",
    "                    b_actions.long()[mb_inds].T,\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if True: # if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if True:  # if clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -clip_coef,\n",
    "                        clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss + vf_coef * v_loss #- ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if False: # if args.target_kl is not None:\n",
    "                if approx_kl > target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "#         '''\n",
    "\n",
    "        \n",
    "    envs.close()\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#         [lambda: HangmanEnv(trainloader) for i in range(trainloader.batch_size)]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wins : 3880 \t total games : 6036 \t win rate : 64.281% \t time_per_game : 23.784 ms \t average reward : 0.000 "
     ]
    }
   ],
   "source": [
    "# from env.hangman import HangmanEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "# agent = Agent(envs).to(device)\n",
    "# optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-5, eps=1e-5)\n",
    "\n",
    "# def test_pgn(dataloader):\n",
    "dataloader = valloader\n",
    "    \n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda: HangmanEnv(dataloader) for i in range(dataloader.batch_size)]\n",
    ")\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
    "\n",
    "\n",
    "agent.train()\n",
    "\n",
    "cr = mini_sim(agent, envs, optimizer)\n",
    "\n",
    "#         print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_ = time.time()\n",
    "test_pgn(trainloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.single_action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(idx):\n",
    "    def thunk():\n",
    "        env = HangmanEnv(dataloader=valloader, init_counter=0)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [lambda: HangmanEnv(valloader) for i in range(3)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 17, 15], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27, 27, 27, 27, 27, 27, 27, 27, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [27, 27, 27, 27, 27, 27, 27, 27, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "       dtype=torch.int32)\n",
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     action = ['a', 'b', 'b']\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a letter to guess: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguessed_letters\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rlenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rlenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "obs, info = envs.reset()\n",
    "print(torch.tensor(obs))\n",
    "print(get_valid_actions(info['guessed_letters']))\n",
    "done = False\n",
    "while not done:\n",
    "#     action = ['a', 'b', 'b']\n",
    "    action = input(\"Enter a letter to guess: \")\n",
    "    obs, reward, terminated, truncated, info = envs.step(action)\n",
    "    print(info['guessed_letters'])\n",
    "    print(get_valid_actions(info['guessed_letters']))\n",
    "    print(info['guessed_letters'].shape)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_sim2(sample):\n",
    "    env = HangmanEnv(sample[0])\n",
    "    n = len(sample[0])\n",
    "    state = masker.mask(sample, 1)\n",
    "    sample_mask, _ = create_masks(tokenizer.encode(sample))\n",
    "    mask = sample_mask.to('cuda')\n",
    "    y = sample_mask.squeeze(1).to('cuda')\n",
    "    y_float = torch.where(y, 1., 0.)\n",
    "    \n",
    "    left = torch.ones((1, 28)).to('cuda')\n",
    "    left[0,  0] = 0.\n",
    "    left[0, -1] = 0.\n",
    "    \n",
    "    P = nn.Softmax(dim=-1)\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    cr = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # print(state)\n",
    "        \n",
    "        state = tokenizer.encode(state)\n",
    "        state = state.to('cuda')\n",
    "        \n",
    "        # q_probs = score / torch.sum(score)\n",
    "        \n",
    "        probs = pgn(state, mask)\n",
    "        \n",
    "        b_probs = torch.mul(probs, left)\n",
    "        b_probs = b_probs / torch.sum(b_probs)\n",
    "        b = torch.distributions.Categorical(probs=b_probs)\n",
    "\n",
    "        action = b.sample()\n",
    "        \n",
    "        # using a greedy approach\n",
    "        guess_id = torch.argmax(b_probs).item()\n",
    "        \n",
    "        # guess_id = action.item()\n",
    "        guess = ids[guess_id]\n",
    "        \n",
    "        next_state, r, done, _ = env.step(guess)\n",
    "        \n",
    "        state = [''.join(next_state)]\n",
    "#         print(state) #, guess, r, next_state)\n",
    "        \n",
    "        left[0, guess_id] = 0.\n",
    "        \n",
    "        cr += r\n",
    "        # print(guess, cr)\n",
    "    \n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedder(\n",
       "  (embed): Embedding(28, 128, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgn.transformer.encoder.embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from env.hangman import Hangman, HangmanEnv\n",
    "\n",
    "def test_pgn2(valloader):\n",
    "    \n",
    "    wins = 0\n",
    "    reward = 0\n",
    "    total_games = 0\n",
    "    pgn.eval()\n",
    "    for i, state in enumerate(valloader):\n",
    "        \n",
    "        if total_games > 10: return\n",
    "        \n",
    "        cr = mini_sim2(state)\n",
    "        if cr > - 6:\n",
    "            wins += 1\n",
    "            # print(state)\n",
    "        total_games += 1\n",
    "        reward += cr\n",
    "        \n",
    "        avg_reward = reward / total_games\n",
    "        win_rate = wins / total_games\n",
    "        print('\\r  wins : %d \\t total games : %d \\t win rate : %.03f%% \\t reward : %.03f \\t average reward : %.03f ' %(wins, total_games, 100*win_rate, cr, avg_reward), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wins : 6 \t total games : 11 \t win rate : 54.545% \t reward : -3.000 \t average reward : -5.364 \n",
      " 1.878509521484375\n"
     ]
    }
   ],
   "source": [
    "t_ = time.time()\n",
    "test_pgn2(valloader)\n",
    "print(\"\\n\", time.time() - t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "trexquant_challenge.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
