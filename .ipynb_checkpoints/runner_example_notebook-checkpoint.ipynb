{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676601a5",
   "metadata": {},
   "source": [
    "# Simulating games of Hangman using a heuristic policy\n",
    "\n",
    "In this notebook, we shall run example games of Hangman using our pre-trained transformer model as shown in the notebook `pre_train_notebook.ipynb` by constructing an intuitive heuristic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7495846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size : 225027\n",
      "Validation size : 2273\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.utils import MyTokenizer, MyMasker, TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# prepare the dataset\n",
    "dataset = TextDataset('./data/words_250000_train.txt')\n",
    "\n",
    "train_split_percent = 0.99\n",
    "train_size = int(train_split_percent * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "print(f'Training size : {train_size}\\nValidation size : {test_size}')\n",
    "\n",
    "# Using the same seed as we did for training\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463d68e",
   "metadata": {},
   "source": [
    "### Importing custom Hangman gym-based env\n",
    "Check out the source code of the environment under `env` directory. \n",
    "* Follows the gym protocol.\n",
    "* Is vectorized and can support multithreading for parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b6b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from env.hangman import HangmanEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5736128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose and split the dataset into `n` buckets for generating words for hangman\n",
    "\n",
    "n = 32  # number of environments to run in parallel\n",
    "\n",
    "dataset = val_dataset\n",
    "n_datasets = random_split(val_dataset, [1/n]*n)\n",
    "\n",
    "def make_envs(dataset):\n",
    "    def thunk():\n",
    "        return HangmanEnv(dataset=dataset)\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_envs(ds) for ds in n_datasets]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f39023",
   "metadata": {},
   "source": [
    "### Implementing our Agent which will interact with the Hangman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa01c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.Models import Transformer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        # pretrained model outputs raw logits of `expected` word from supervised learning\n",
    "        self.pretrainedLLM = Transformer(src_vocab=28, d_model=128, max_seq_len=32, N=12, heads=8, dropout=0.1)\n",
    "        self.pretrainedLLM.load_state_dict(torch.load('./weights/aaa_best_weights'))\n",
    "        \n",
    "    def act(self, x, guessed_letters):\n",
    "        valid_actions = self.get_valid_actions(guessed_letters)\n",
    "        \n",
    "        mask = (x != 0).unsqueeze(-2)\n",
    "        logits = self.pretrainedLLM(x)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        probs = torch.matmul(1.*mask, probs)  # effectively adds the probs row-wise for each action / character\n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs / torch.sum(probs)\n",
    "\n",
    "        fprobs = torch.mul(probs, valid_actions)  # zero out probabilites of invalid actions\n",
    "        action = torch.argmax(fprobs, dim=-1)  # follow a greedy hueristic based policy on letter frequency\n",
    "        \n",
    "        return action, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_valid_actions(guessed_letters):\n",
    "    \n",
    "        valid_actions = torch.ones((len(guessed_letters), 28)).to('cuda')\n",
    "        valid_actions[:,  0] = 0.\n",
    "        valid_actions[:, -1] = 0.\n",
    "\n",
    "        for i, s in enumerate(guessed_letters):\n",
    "            for char in s:\n",
    "                idx = ord(char) - ord('a') + 1\n",
    "                valid_actions[i, idx] = 0.\n",
    "\n",
    "        return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10055471",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (pretrainedLLM): Transformer(\n",
       "    (encoder): Encoder(\n",
       "      (embed): Embedder(\n",
       "        (embed): Embedding(28, 128, padding_idx=0)\n",
       "      )\n",
       "      (pe): PositionalEncoder(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x EncoderLayer(\n",
       "          (norm_1): Norm()\n",
       "          (norm_2): Norm()\n",
       "          (attn): MultiHeadAttention(\n",
       "            (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (linear_1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Norm()\n",
       "    )\n",
       "    (out): Linear(in_features=128, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "agent = Agent().to(device)\n",
    "agent.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34195bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins : 1590 \t total games : 2404 \t total unique games : 2273 \t win rate : 66.140%\n",
      "--------------------------------------------------------\n",
      "True win rate \t\t\t: \t 66.432%\n",
      "Time take to run 2404 games \t: \t 12.715 s\n",
      "Mean time per game \t\t: \t 5.289 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# dictionary of results per given hidden word\n",
    "wins = {}\n",
    "total_games = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "state, info = envs.reset()\n",
    "state = torch.tensor(state).to(device)\n",
    "\n",
    "print_counter = 0\n",
    "while True:\n",
    "    action_ints, _ = agent.act(state, info['guessed_letters'])\n",
    "    \n",
    "    # Convert to action_int to action_str guesses\n",
    "    action_strs =  [chr(idx-1 + ord('a')) for idx in action_ints]\n",
    "    \n",
    "    # Take step in the envs\n",
    "    state, reward, terminated, truncated, info = envs.step(action_strs)\n",
    "    state = torch.tensor(state).to(device)\n",
    "    done = (terminated | truncated)\n",
    "    \n",
    "    # Print running statistics\n",
    "    if done.any():\n",
    "        for hidden_word, r, d in zip(info['hidden_word'], reward, done):\n",
    "            if int(d) == 1:\n",
    "                total_games[hidden_word] = total_games.get(hidden_word, 0) + int(d)\n",
    "                wins[hidden_word] = wins.get(hidden_word, 0) + int(r)\n",
    "                \n",
    "                if print_counter % 50 == 0:\n",
    "                    print('''\\rwins : %d \\t total games : %d \\t total unique games : %d \\t win rate : %.03f%%''' \\\n",
    "                          %(sum(wins.values()), sum(total_games.values()), len(total_games), 100*sum(wins.values())/sum(total_games.values())), end='', flush=True)\n",
    "                print_counter += 1\n",
    "                \n",
    "    if len(total_games) == len(dataset):\n",
    "        print('''\\rwins : %d \\t total games : %d \\t total unique games : %d \\t win rate : %.03f%%''' \\\n",
    "          %(sum(wins.values()), sum(total_games.values()), len(total_games), 100*sum(wins.values())/sum(total_games.values())), end='', flush=True)\n",
    "                \n",
    "        end_time = time.time()\n",
    "        break\n",
    "\n",
    "# The true win rate is a better metric\n",
    "true_win_rate = 0\n",
    "for word, win in wins.items():\n",
    "    true_win_rate += win / total_games[word]\n",
    "\n",
    "# PRINT RESULTS\n",
    "print('\\n--------------------------------------------------------')\n",
    "print(f'True win rate \\t\\t\\t: \\t {100*true_win_rate/len(total_games):.03f}%')\n",
    "print(f'Time take to run {sum(total_games.values())} games \\t: \\t {(end_time-start_time):.03f} s')\n",
    "print(f'Mean time per game \\t\\t: \\t {1000* (end_time-start_time) / sum(total_games.values()):.03f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56eac4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We can see that our model with the heuristic policy performs fairly well even on the validation dataset. However, one can further try improving the model by using Reinforcement Learning techniques. This can be a future area of research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
